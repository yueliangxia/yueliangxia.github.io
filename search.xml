<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[WeiBoSpider]]></title>
    <url>%2F2018%2F09%2F14%2FWeiBoSpider%2F</url>
    <content type="text"><![CDATA[关于发布时间转成统一格式的问题 现在发现有三种格式的时间 **分钟之前(一小时之内的会这样显示) 今天 :(超过一小时就会这样显示) 09月13日 22:33 (不是今天的) 现在考虑的是,分别格式化为时间戳,然后再从时间戳转换成要求的统一格式]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>scrapy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[proxy_pool]]></title>
    <url>%2F2018%2F09%2F12%2Fproxy-pool%2F</url>
    <content type="text"><![CDATA[代理IP: 1.免费的代理IP,可用IP少,时效性较短,大部分的代理IP可能在访问网站都是失效的. 2.收费的代理IP,讯代理. 3.爬取国外的网站,VPN代理服务器. proxy_pool: @项目地址：https://github.com/jhao104/proxy_pool.git 1.它将国内的代理IP网站都进行了爬取; 2.代理IP爬取完毕之后,会进行检测,可用的IP会保存到数据库redis中; 3.会定期将数据库中的代理IP拿出来检测,失效的IP从数据库中删除。 4.支持扩展 练习项目:1. 解决登陆问题； 2. 使用downloadmiddleware, ua, 代理、池; 3. 使用itemloader完成数据的提取和解析； 4. 提取所有问题的信息：标题、点赞数、评论数、关注者数量、浏览数量；将列表页数据单独 保存至一个表中； 5. 提取每一个问题的所有答案：回答用户、回答内容、回答时间、回答 点赞数、回答评论数；将所有的答案保存至另外一个表中； 6. 数据库的写入要求使用异步Twisted框架完成； 首先需要先设置好代理IP和请求头,也就是自定义下载中间件然后根据需求知道要先获得登录后的Cookie 这个设置在middlewares里 cookie登录需要设置成键值对的方式 1234567891011121314request.cookies = &#123; '_xsrf':'BKORzjqUj35ax0DdrwKnVn25RkNRaahM', '_zap':'4dc36192-e267-44d1-b8f4-074822456258', 'd_c0':"AFAlC-xzMw6PTlUrgV6b_Bu4DtkilyXo-Z4=|1536751108", 'capsion_ticket':"2|1:0|10:1536756874|14:capsion_ticket|44:MDRlODI3NDk5ZjhlNGRlOThiNzdiN2Q1NGM3OWEyNjQ=|7b1b74e68e3ccc4440577f394f8e1ded1efc6a5bca537a1a0d459b71d36877c2", 'q_c1':'cd8e72dfa9384e70af51bc5c90b10f8a|1536751149000|1536751149000', 'l_n_c':'1', 'l_cap_id':"NGRjMmM4OTZkOTBlNGQ0ZTg2MTQ2YTk4YTNiODBhMGI=|1536756854|ae8c593677b5d5d7301941fa6e133c0c4f4f1022", 'r_cap_id':"YTM4YTAyZjQ2ZDAwNDhiNjlmYTYxNmFmODU2NTMwMWU=|1536756854|b164129500acaa359cfc585c4c1d5d32b8529bf2", 'cap_id':"MjQ0ZWZhOGQwZDlhNGQ5MGE0ZWRlMTNlMWYyZDcwMWQ=|1536756854|a8f854ab2430dbb1dc0f0ee8b9cf89e9f316b556", 'n_c':'1', 'z_c0':"2|1:0|10:1536756886|4:z_c0|92:Mi4xU2JyYkF3QUFBQUFBVUNVTDdITXpEaVlBQUFCZ0FsVk5sbHFHWEFEMGZOMm5qMmItXzJlUElVWEpCWHZGWkFzMkt3|8e0843029fb42283f58a63ec8765ba96c10b936e8775f14293cf3b12501e0f13", 'tgw_l7_route':'53d8274aa4a304c1aeff9b999b2aaa0a' &#125; 登录成功之后,获取问题详情, 1.发现获取详情的网址是xhr请求,其中一条为https://www.zhihu.com/api/v3/feed/topstory?action_feed=True&amp;limit=7&amp;session_token=b36c08db4577ef8d563788f5337b283c&amp;action=down&amp;after_id=13&amp;desktop=true 2.经解析参数发现session_tokon这个参数,每次启动项目都会不一样,所以尝试在主页面的response中寻找.最终解析该值得代码为: 1234data = response.xpath('//div[@id="data"]/@data-state').extract_first('')pattern = re.compile(r'sessionToken":"(.*?)"',re.S)session_token = re.findall(pattern,data)print(session_token) 3.参数after_id的值为每一次加7,所以设置range循环,步进为7 yield回调解析json数据的函数.经测试发现有的问题,没有question的值.–这个问题是因为: 有的是文章,有的是问题.–这里暂时只解析那些是问题的页面. 网址都是跟据id拼接出来的 然后发现https://www.zhihu.com/question/41435689/answer/465837326拼接出这样的网址是无法获取到全部的回答的.所以..还是考虑把后面的回答者去掉 变成https://www.zhihu.com/question/41435689这样 解析问题页面的评论 1.https://www.zhihu.com/api/v4/questions/41435689/answers?发现xhr请求中类似这样的get请求是获取的评论. 2.进去后经简单解析,发现没个请求有四个,所以就用从0开始步进为5的循环.–以后可以考虑获取一下评论的总数,然后按总数获取. 这一点参数里有两个值offset=5&amp;limit=5这两个控制每次获取的评论数.暂时不需要更改 3.include=data%5B*%5D这个参数,好像没个问题的也都不一样. 关于这一点,参数里面需要这些东西的都是用[]代替的,写出[]也能正常获取评论 4.scrapy获取的json数据我用的解析方式,这样就能获取到字典了 12response_data = response.body.decode('utf-8')json_data = json.loads(str(response_data)) 5.关于评论帖子的时间这一块,返回的好像是个时间戳,需要反序列化一下 123456789import timeif 'updated_time' in data: # 有更新时间就获取更新时间 date = data.get('updated_time','')else: # 没有就获取创建时间 date = data.get('created_time','')timeArray = time.localtime(date)date = time.strftime("%Y-%m-%d %H:%M:%S", timeArray) 关于遇到的一些错误1MySQLdb TypeError: %d format: a number is required, not str问题解决 这个其实就是传参数的时候占位符填错了,一般都是因为传int类型时不需要用%d,直接用%s就搞定了,亲测成功~! 还有一点就是最好要把str类型的占位符改成’%s’,就是带引号的这种,防止1146错误..]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>scrapy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[用twisted异步写入]]></title>
    <url>%2F2018%2F09%2F11%2Fuse_twisted%2F</url>
    <content type="text"><![CDATA[异步写入MySQL数据库,如果scrapy解析数据的速度远远超过数据库的写入速度,那么很容易就造成数据的丢失.这里用twisted保证数据的异步多线程写入,提高写入速度.详情请看正文@本文相关源码: https://github.com/Loveyueliang/yuke/tree/master/9-11/jobbolespider 首先需要先导入模块1from twisted.enterprise import adbapi 接着在自定义的pipeline里面重构from_settings函数12345@classmethoddef from_settings(cls, settings): """ 这个函数名称是固定的，当爬虫启动的时候，scrapy会自动调用这些函数，加载配置数据。 """ 创建一个数据库连接池对象，这个连接池中可以包含多个connect链接对象。123# 参数1：操作数据库的包名# 参数2：链接数据库的参数db_connect_pool = adbapi.ConnectionPool('pymysql', **params) 初始化这个类的对象1obj = cls(db_connect_pool) 在process_item这个函数里执行数据的多线程写入操作12345# 参数1：在线程中被执行的sql语句# 参数2：要保存的数据result = self.dbpool.runInteraction(self.insert, item)# 给result绑定一个回调函数，用于监听错误信息result.addErrback(self.error) 定义一个打印错误信息的函数 12def error(self, reason): print('--------', reason) 定义一个执行插入语句的函数,用twisted的好处其中一点就是不用一步步的手动提交 1234def insert(self, cursor, item): insert_sql = "INSERT INTO bole(bole_title, bole_date, bole_tag, bole_content, bole_dz, bole_sc, bole_pl, bole_img_src) VALUES (%s, %s, %s, %s, %s, %s, %s, %s)" cursor.execute(insert_sql, (item['bole_title'], item['bole_date'], item['bole_tag'], item['bole_content'], item['bole_dz'], item['bole_sc'], item['bole_pl'], item['bole_img_path'])) # 不需要commit()]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>scrapy,mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于51job的爬虫说明]]></title>
    <url>%2F2018%2F09%2F10%2Ftest_pic%2F</url>
    <content type="text"><![CDATA[@本文源码：https://github.com/Loveyueliang/yuke/tree/master/9-10 错误点: 感觉唯一的坑就是刚学的sql插入,一开始定义的表的字段名和item定义的不一样,所以就各种报KeyError.12insert_sql = &quot;INSERT INTO job(zwmc, zwxz, gzdd, gzyq)VALUE (&apos;%s&apos;, &apos;%s&apos;, &apos;%s&apos;, &apos;%s&apos;)&quot;%(item[&apos;zwmc&apos;],item[&apos;zwxz&apos;],item[&apos;gzdd&apos;],item[&apos;gzyq&apos;]) self.cursor.execute(insert_sql) 最终结果还不错,暂时就爬取这一点信息吧.]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>scrapy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[xieyuying]]></title>
    <url>%2F2018%2F09%2F07%2Fxieyuying%2F</url>
    <content type="text"><![CDATA[@本文源码：https://github.com/Loveyueliang/XieYuYing.git si: 这里面写一写个人需求或建议的一写东西.调用剧情,选择用1-11这种类似的东西关于人物类的创建,人物可以考虑做在一个文件里,共同继承一个父类People –现在这一项已经完成.关于玩家名字的说明: 调用更改玩家名字的时候, 直接用赋值语句,现有判定输入只能为中文名字关于好感度的说明:好感度设计的逻辑一开始没有理清.现已完成 1.0版本–定义了一个好感度的类,方法为增加/减少好感度,目前是女主和胖老板的父类.理想参数为执行的相应动作,返回值为增加/减少的好感度. 但是目前还没有定义需要的所需要的以动作和数字为键值对的字典.动作多的话倒是可以采用这种方法, 要是很少的几个动作,估计要重新给每一个动作编写一个方法 好感度采用等差数列递增的方式.多引用一个增长速度 2.0最新进度,可以采用函数右移的方法来实现偶尔的坏话处理,,把函数做成列表生成器,达到每调用一次就可以得到下一个结果 生成器也做好了,但是需要一个初始的好感度,也就是说,,先给每个人用生成器生成到相应想给到的好感度 3.0最终版本:终于搞定了 好感度最终定义为一个列表增加好感度的方法里面先查询现在好感度的索引,然后通过索引加1的方法调用下一个应该得到的好感度. 关于商品的说明: 现在还没有开始做不过估计要创建一个商品的类,方法的话,应该就只有购买,不过属性可能会有点多. 需要注意: 要添加的一个功能–通过购买可以选择删除剧情中的某个人物,感觉关联度有点高]]></content>
      <categories>
        <category>类的使用</category>
      </categories>
      <tags>
        <tag>面向对象</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机开机]]></title>
    <url>%2F2018%2F09%2F06%2Fcapture_start%2F</url>
    <content type="text"><![CDATA[计算机启动过程 BIOS简介: 基本的输入输出系统,连接底层的硬件和系统 第一步: 按下电源键 按下电源键的时候电源开始向主板和cpu供电,一开始电压不稳定,主板控制芯片会一直向cpu发送reset信号,让cpu初始化 一般这个过程非常短暂,几乎在一瞬间电压就会稳定,控制芯片撤掉reset信号.但按下reset的时候,这个过程就特别明显…. 第二步: BIOS硬件自检–检测关键性设备 BIOS开始上电自检,这个地方会主要会检测内存,显卡,要是检测不通过,蜂鸣器会报告检测到的错误,一般用滴的长短和次数来表示错误的类型 这个地方检测的内存地址只有640k. 第三步: 屏幕亮,检测显卡BOIS 这里系统BIOS会读取显卡的BIOS启动显卡,此时屏幕开始亮起,会简单的显示显卡的一些参数, 要是不亮,就… 第四步: 显示BIOS自己的界面 会显示BIOS日期,主板芯片组型号,主板识别码,主板厂商,等一堆东西. 第五步: 仔细检测cpu和内存 会把检测到的cpu类型,主频打印在屏幕上 下面会跟着开始检测所有内存信息 第六步: 检测标准硬件设备 检测硬盘 cd-rom 软驱 并行接口,和串行接口 第七步: 检测外围设备 检测那些可以即插即用的东西,比如usb接口的U盘什么的.检测到后为每一个检测到的设备分配dma.i/o接口等 每检测到一个设备,都会把信息打印在屏幕上 第八步: BIOS清屏,重新显示系统配置表 会在系统上方显示一个概括性的配置列表,列出了系统安装的各种各种标准硬件设备以及他们使用的资源和相关参数. 第九步: 检测硬件更新 这里的硬件指的是类似外接内存,声卡,网卡,显卡,等设备有没有被更换,如果更换了,就更新一下存储,方面系统读取配置. 第十步: 根据启动顺序,启动引导 这一步,才开始引导操作系统,系统BIOS读取并执行硬盘的主引导记录,从分区表中找到第一个活动分区,然后读取并执行分区的引导记录.分区的引导记录负责读取并执行系统 下面是结构图]]></content>
      <categories>
        <category>前端</category>
      </categories>
      <tags>
        <tag>capture</tag>
      </tags>
  </entry>
</search>
