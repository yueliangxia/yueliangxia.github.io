<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Python基础中的一些重点]]></title>
    <url>%2Farticle%2Fc7ae822f%2F</url>
    <content type="text"><![CDATA[对于编程语言来说，在基础语法这一块Python还是相对简单的。不过也有很多知识容易记不清或者干脆直接忘记，所以在这里稍微总结一下。以后根据情况会向里面添加新的东西的。写在前面声明一下：这里不针对每一个人，写的东西只是我个人人文不太好记的东西。当然了，大家能帮我提供一些知识点也是极好的，毕竟我的思维也有限。而且，由于思路可能并不是那么清晰，可能稍微会有点乱。望大家海涵！首先是基础语法。关于序列化和反序列化这一点，我觉得的还是很容易会被搞混的，我就说一下我的看法。首先，我感觉要先明白序列化的具体定义的话这一块基本上就记住了。这里是引用百度百科的一句介绍：序列化 (Serialization)将对象的状态信息转换为可以存储或传输的形式的过程。在序列化期间，对象将其当前状态写入到临时或持久性存储区。以后，可以通过从存储区中读取或反序列化对象的状态，重新创建该对象。这句话简单一点理解就是说，序列化是为了使自定义对象什么的可以存储起来，而反过来读取那些存好的数据都叫做反序列化。理解了这一点后面就简单了，首先loads()从字面意思上就能看出来这是要读取或者说加载括号里面的数据，那根据上面的理解也就是反序列化的意思；同理，dumps()有临时堆积处、临时仓库的意思，也就是说暂时先把数据存储起来，也就是序列化的意思了。时间与日期相关这里面有两个函数容易记混：格式化时间和日期元组（strftime）：意思是 str from time 从时间元组中得到的字符串，也就是将时间元组格式化为字符串。反过来：将格式化的字符串转化成时间元组（strptime）：意思应该是 str print time 将字符串打印成时间元组。内存管理机制Python是自动管理内存的，方式采用的是“引用计数”：对一个对象的生命周期进行管理。如果引用计数为0，对象就会被解释器进行内存的回收，对象会在内存中消失；如果不为0，则会一直存在函数有关常见的装饰器： property、classmethod、staticmethod赋值-浅拷贝-深拷贝这里是大佬的一篇文章：一切都是对象，一切都是指针，一切都是东西（python的编程哲学），我觉得他讲的内容对加深Python的理解非常有用。1 2 3 import copycopy.copy()copy.deepcopy()因为一切都是指针，所以赋值也就只是指针的赋值，只是给变量名指定了新的指针。浅拷贝： 只把对象本身拷贝了一个新的，对象里面引用的其它指针都没有拷贝。深拷贝： 拷贝了一份完全新的对象，跟原来的没有一点关系。关于高阶函数Python内置了一些特殊的函数，这些函数可以用函数当参数。这里只简单说一下map()函数、filter()函数、reduce()函数和sorted()函数。这map、filter、reduce这三个函数的用法基本都是一样的，同样都是接收两个参数：第一个是函数，第二个是列表吧。 我还是弄个简单的表格吧。函数第一个参数(一参)第二个参数(二参)处理方式返回值map带有一个参数的函数一般是可迭代对象把二参的数据分别取出交给一参处理map类型，需要列表的话需要用list()函数自己转换。filter带有一个参数的函数一般是可迭代对象分别对二参里的值进行一参里的判断为True时保留二参的值，filter类型，也可自己转换为list。reduce带有两个参数的函数一般是可迭代对象类似斐波那契数列依次取两个数处理字符串类型sorted一般是可迭代对象key(可选)=可以是匿名函数对可迭代对象进行排序key是指定按某一个值排序，reverse(倒序)默认为False,返回值为网络方面的一些知识get和post请求的区别get和post的区别其实谁都能说出来两条，可总是会漏点什么东西。对了这里说一下常见的各种请求：get、post、head、ssl、socket首先是请求的网址：get请求的数据会附加在URL之后，并且以？分隔URL和传输的结果，参数之间用&amp;链接；post请求的url则不会有参数，提交的数据都在HTTP包的包体重，并不会暴露。总结一下就是：get提交的数据会在地址栏显示出来，post则不会既然post请求不会显出来参数，那接下来就是安全性方面：post请求安全性更高的，比如登录界面被浏览器缓存也就是保存历史记录，那么其他人查看历史记录直接就能看到。一般都会忘了这一点，传输数据的大小：因为看上去两个请求在这一点是没什么区别的，平时也不用注意，但就是不一样：因为get请求参数会加长url长度，所以特定的服务器对URL长度有限制，比如IE对url长度的限制是2048字节。post请求提交的数据大小有的服务器也会进行限制。URL基础一些最基础的知识，就当复习吧。还有一点很不好记：HTTP请求报文组成： 请求行 + 请求头部 + 空行 + 请求数据对应的 HTTP响应报文： 状态行 + 响应头 + 空行 + 相应数据URL(统一资源占位符),是对可以从互联网上得到的资源的位置和访问的一种简介的标识。是互联网上标准资源的地址。URL的组成： 协议(或者叫服务方式http、https) + 存有资源的主机的IP地址(有时候会带有端口号) + 主机资源的具体地址,如目录或者文件名等. + (可选参数)一些常见的端口号：端口名端口号HTTP80HTTPS443一般邮箱25QQ邮箱(SSL)465数据库3306微信80爬虫的一些东西关于爬虫的东西就有点多了，这里不知道能写多少。如有需要可以看看我的其它具体写的文章，万一有了想找了的呢。]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[鸡声茅店月，人迹板桥霜]]></title>
    <url>%2Farticle%2F526b2bdf%2F</url>
    <content type="text"><![CDATA[]]></content>
  </entry>
  <entry>
    <title><![CDATA[关于新手购买域名和选择服务器的说明]]></title>
    <url>%2Farticle%2F21811%2F</url>
    <content type="text"><![CDATA[最近身边的好多小伙伴都购买了域名，但是不知道该怎么部署才能实现装逼效果，下面我就简单的写一点相关的说明和我遇到的一些坑。写的不好请见谅。必读 写在前面本文的内容是购买域名和相对应域名商的解析设置，然后搭建的过程会贴出大佬的文章链接。首先当然是考虑先买一个域名啦。服务器什么的慌什么，也跑不掉。现在好多的服务商都提供域名购买服务，其实买到的域名都是一样的，就是各大服务商的政策有一点区别。1.首先各大服务商基本都有域名购买服务。比如阿里云，腾讯云，百度云···等2.这里说一下购买步骤。在云服务商主页，这里采用的是腾讯云的例子。一般找不到的话上面都有搜索，输入域名。如下图后面的步骤这里就不演示了，花钱买就行了。不过要说一下，要是第一个域名的话，腾讯云的会有一个活动，不是的话就算了。服务器提供商和博客框架的选择域名购买之后就是要选择服务器提供商了。虽然说国内各大网站卖的肯定是各种服务器都有，但是1.贵，2.都需要备案。所有暂时都不考虑服务器：这里只说一下我比较了解的两个没有以上两点问题的服务器：1.GitHub Pages的部署相关; 2.Nc主机的部署相关博客框架：这里也只说两个，一个是无后台的Hexo，一个是有后台管理界面的WordPress。选择前须知：要是喜欢部署时稍微简单点的，建议先选择nc主机，然后搭配WordPress框架，但其实我感觉难易程度总体来说是差不多的。反正个人也就是我现在用的是GitHub Pages搭配的Hexo框架。理论上说，各个服务器能搭配的框架都是很多的，但还是建议根据上面的搭配来搞。GitHub Pages的部署相关开始搭建博客：在本地安装HEXO，建议阅读一遍官方文档，当然不想看了，就看下面的步骤也行.安装前需要的必须组件：Git和Node.js。这两个都直接去官网下载安装就行了，这里就不写步骤了，毕竟网上有好多。而且只需要一直下一步就行了。Node.js安装时一定要注意Custom Setup这一步要选Add to PATH添加到环境变量开始安装Hexo：所在目录：~/blog/1 2 3 4 5 6 7 8 # 安装hexonpm install hexo-cli g# 初始化博客文件夹hexo init # 安装hexo的扩展插件npm install# 安装其它插件...上面的步骤，每一步都需要耐心等待。暂时还没发现什么好方法。如果正常走完的话，恭喜你这个框架已经安装完成了！–hexo init时，如果出现橙色的WARN请无视，如果出现红色的EERROR，请自行查找方法。所在目录: ~/blog/1 2 3 # 测试是否安装完成：hexo g # 生成一个包含所有静态网页所需东西的文件夹publichexo s # 通过自带的API接口，让你可以本地预览再在本地打开http://localhost:4000/，如果现实有欢迎页面，恭喜你。第一步搞定了。本地部署好了，当然要开始绑定自己的域名啦。GitHub Pages是由GitHub提供的静态网页服务。官网地址为https://pages.github.com/，有兴趣的可以去瞅瞅。不过人家是全英的。。。注册GitHub 帐号和创建 Repository仓库请看这篇文章：基于Hexo+github+coding搭建个人博客——基础篇(从菜鸟到放弃)，只看这两步就行了，不想再写一遍了。两步走完之后在接下来出现的页面找到我第一个箭头的位置点一下，改成如下图所示，把网址复制出来复制到下面接着的第二步里。言归正传：继续讲怎么部署到GitHub。1.首先在下面目录打开Git Bash安装下面这个插件所在目录: ~/blog/1 npm install hexo-deployer-git --save # 用来部署到git的插件2.然后，打开站键文件夹根目录的配置文件_config.yml，在大概是最后的位置更改：文件位置：~/blog/_config.yml1 2 3 4 5 6 7 8 9 10 省略。。。# Deployment## Docs: https://hexo.io/docs/deployment.htmldeploy: type: git repo: # 下面写你自己的github的地址。也就是刚才让复制的地址。 github: git@github.com:yueliangxia/yueliangxia.github.io.git branch: master3.为了不让自己每一次部署都输入账号密码，就需要下面的步骤，也就是绑定SSH Key。对的没错，还是上一篇文章，不过我帮你你换了个索引以便于快速定位到这个地方。请按照人家的步骤绑定一下。基于Hexo+github+coding搭建个人博客——基础篇(从菜鸟到放弃)4.上面步骤完成之后，就算是大功告成了。所在目录：~/blog/1 2 hexo g # 生成静态文件hexo d # 部署到githubhexo d第一次部署的时候会弹出一个输入框，输入yes就行了。之后请进入github并找到自己的用户名.github.io的仓库打开，查看是否有文件。如果有文件请点击setting。进去后，请在第一个框里写上自己的域名，并把第二个启用HTTPS的框勾上，如下图特别注意： 关于部署完成之后服务器和域名跳转问题。需要在source文件夹里新建一个名为CNAME的文件，注意这里没有.txt后缀，什么后缀都没有，里面只需要写上自己的域名就行了，另存为utf-8格式。重新hexo d一下吧！Hexo 优化相关关于主题的更换：首先当然说一下自带的主题。。来看一下自带的主题官网，不翻墙可能有点慢，不过也能看。这里面带有几乎所有支持的主题，点击图片可以查看预览。如果想要应用的话，就需要点击主题的名字，进入github的界面。因为这些主题都是存放在github的更换主题的命令所在目录：~/blog/1 2 3 git clone github提供的下载地址 themes/主题的名字# 下面是其中一个的栗子git clone https://github.com/probberechts/hexo-theme-cactus.git themes/cactus然后更改站点配置文件文件位置：~/blog/_config.yml1 2 3 4 5 省略。。。## Themes: https://hexo.io/themes/# 把下面的主题名字改成自己想用的theme: 主题的名字这里推荐几个不错的主题：1.我当前在用的next2.yilia3.indigo4.cactus,选择主题就说到这里，就靠自己的喜好了。Nc主机的部署相关这个主机嘛，提供的有免费虚拟机和的一个免费的大小还凑合数据库。说的是距离近，但是不花钱，访问速度也就比github提供的稍微快一点点。不明显！1. 首先去人家的官网http://idc.ncshu.com/会员中心自己注册个账号。2. 接下来去官网选择免费主机，然后选香港免费主机，点击下图立即订购3. 配置账户界面 ：这个控制面板密码将会是你的ftp访问和数据库访问的密码。建议浏览器自动保存一下，不然忘了又麻烦。这里建议先写上域名不然的话就自己去控制面板选择绑定域名。4. 控制面板：搞好之后进入会员中心，然后选择已购产品-主机管理–然后进入管理面板5. 上传文件：先去WordPress官网下载这个框架的包。当然，下载地址我贴出来了https://wordpress.org/latest.zip。下载完成后记住地址一会儿把这个框架上传到NC主机回到NC主机的控制面板–选择FTP/文件管理–在线文件管理器–然后在右侧选择上传文件–选择刚才下载的压缩包文件上传上传完成后，目录结构如下图，选择解压，解压目录写上/wwwroot,密码写控制面板的密码6. 回到基本功能–默认文件–文件名写上index.php，顺序写100–提交7. 绑定域名：首先按照提示 增加域名绑定，在添加绑定之前请先解析域名：A记录到IP xxxx。这一点请查看下面域名绑定相关，绑定好后进行下一步域名填写自己买的域名，目录写 wwwroot/wordpress/。—确定如果没什么问题的话，你刚才填上的域名就能访问了。访问自己的域名下面就是正常的WordPress安装界面7. WordPress的安装：选择简体中文–然后继续这里的用户名密码，是你的网站的后台管理员的用户名密码，这个一定要记着。准备好上面的东西，然后–现在就开始然后让你输入数据库名，用户名密码什么的。实际上需要输入的也就只有这两个—不知道的话，打开主机控制面板首页–右边那一列有数据库名，数据库用户名然后选–现在安装，后面的话访问域名，基本的网页就出来了。。如果需要定制，后面再说，或者自己去找相关教程。域名绑定相关这里只说一下注意事项吧：进到买到的域名管理界面都会有个解析设置，或者叫云解析什么的。解析可以设置为A记录指向给的IP比如192.168.**.**，CNAME记录指向给的域名比如***.github.io然后前面可以写@，就是说不带www的访问；可以写www，就是带www的访问。建议这两个都写上然后跳转规则写一样的就行了。]]></content>
      <categories>
        <category>域名和服务器</category>
      </categories>
      <tags>
        <tag>IP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[天眼查的字体加密]]></title>
    <url>%2Farticle%2F8131%2F</url>
    <content type="text"><![CDATA[关于一些网站的字体加密方面，目前发现的也就只有两种：一种是起点网的那个总字数那种的给的是是禁止的数据，转换成十六进制之后再在woff文件转换成的xml文件中寻找对应的关系。另一种就是类似于天眼查的，获取到的源码是字体的位置信息，然后需要根据位置信息取出对应的数据。前提：这里我用了两种方式来搞定这种相对应位置的字体加密。1.第一种的思路是：主要采用PIL图像处理模块来根据1-0这个十个源码里获得的数字取出字体文件里真正要显示的数字，并画在画布上保存成文件，然后利用图片识别(tesseract-orc)技术来取出保存的图像中包含的数字，从而获得它们的对应关系。1.1：首先当然是需要安装必须的工具，俗话说“工欲善其事，必先利其器”嘛。Win系统下首先要安装这个 tesseract-ocr.exe 软件，下载地址就靠自由发挥了。(这个软件需要手动把目录添加到环境变量)对于Python而言，需要装一个中间模块，用来在py文件中调用上面的软件。（这个好装：pip install pytesseract）1.2：下面是这种思路用到的代码1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 def getmapping(font_url): a_time = time.time() # content = requests.get('') # 保存woff文件 # 请求字体文件的url，获取字体文件的内容。 font_response = requests.get(font_url, headers=&#123; 'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100 Safari/537.36'&#125;).content with open('tianyan.woff', 'wb') as f: f.write(font_response) # 1.先定义位置数字 position_number = ['1','2','3','4','5','6','7','8','9','0'] # 2.遍历位置数字列表 map_dict = &#123;&#125; for p_num in position_number: # 3.创建一个图片对象，用于后续数字图片的存储 img = Image.new('RGB',(300,500),(255,255,255)) # 4.根据这个img对象，创建一个画布，用于后续将数字画到画布上 draw = ImageDraw.Draw(img) # 5.根据位置数字p_num和字体文件woff，找出p_num在woff文件中的真实映射关系。 # 参数1：指定字体文件 # 参数2：这个数字在画布上，展示的大小 # truetype()返回一个字体对象 font = ImageFont.truetype('tianyan.woff',400) draw.text((10,10), text=p_num, font=font, fill=ImageColor.getcolor('black','RGB')) # img.show() img.save('tianyan.png') # 6.利用pytesseract包识别图片中的数字 # pip install pytesseract # a&gt; 打开图片 font_image = Image.open('tianyan.png') # b&gt; 调用识别函数 num = pytesseract.image_to_string(font_image,config='--psm 6') if num == 'A' or num == '/': num = pytesseract.image_to_string(font_image,config='--psm 8') map_dict[p_num] = num elif num == "": # 有的字体文件中是缺少部分数字的。那么得到的图片就是一个空白图片，也就是位置数字对应的真实数字不存在。 # 凡是字体不存在的，都有一个特征：就是位置数字和真实数字是相等的。 map_dict[p_num] = p_num else: map_dict[p_num] = num # print('位置：&#123;&#125;，真实：&#123;&#125;'.format(p_num, num)) print('tesseract处理时间：',time.time()-a_time) return map_dict2.第二种思路就是还是解析字体转换成的xml文件，从中找到它们之间的对应关系。这次就主要先写这一个吧。代码我还是直接在代码中注释出来吧。虽然不一定适合其他网站，但是思路还是不错滴。解析转换成xml的woff文件代码1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 from fontTools.ttLib import TTFontfrom xml.etree import ElementTreemap_dict = &#123;&#125;def parse_font_xml(font_url): # 首先 全局定义的有一个空字典，用于存放已经解析过的字体规则。如果已经解析过，则直接取出并返回，提升运算速度。 if font_url in map_dict: return map_dict[font_url] # position_number这个列表定义的是映射前的字符，也就是指源代码中可能获取到的数字。 position_number = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '0'] # 请求字体文件的url，获取字体文件的内容并存储到本地。 font_response = requests.get(font_url, headers=&#123; 'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100 Safari/537.36'&#125;).content with open('tianyan.woff', 'wb') as f: f.write(font_response) # 存好之后，用TTFont打开，并保存成xml文件 f = TTFont('tianyan.woff') f.saveXML('tianyan.xml') # 解析xml标签结构，并取出一个含有所有数字的长度为十的列表命名为maps_element.(最下面有此段的xml代码，但只是众多规则中的一种，仅供参考) root_obj = ElementTree.parse('tianyan.xml').getroot() maps_element = root_obj.find('GlyphOrder').findall('GlyphID')[2:12] # 有的规则会获取不到完整的十个数字，因为有的映射关系没有改变（比如3-3），这样的情况在上面的列表中就不会显示，所以要进行下面的两个步骤 # 1.先遍历maps_element判断取出的值是否为数字，并把是数字的存到新的列表sort_num中。 sort_num = [] for i in range(len(maps_element)): if maps_element[i].attrib['name'] in position_number: sort_num.append(maps_element[i].attrib['name']) # 2.然后判断sort_num这个列表长度是否为十，为十的话，就根本不用处理了 if len(sort_num)!=10: # 如果不为十，就要在映射前的列表中取值，看看映射后的列表也就是sort_num中缺少了什么用insert插入相对应的位置 for num in position_number: if num not in sort_num: sort_num.insert(int(num),num) # 处理完成之后，映射后的数字就按照正确的顺序存在了sort_num这个列表中 # 最后一步就是把值和对应的索引取出来并放在字典中，返回出去 sort_num_dict = &#123;&#125; for key,value in enumerate(sort_num): sort_num_dict[str(value)] = key # 当然不能忘了把字体url和对应的解析规则存放起来，以便下次重复时调用 map_dict[font_url] = sort_num_dict return sort_num_dict1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 &lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;ttFont sfntVersion="\x00\x01\x00\x00" ttLibVersion="3.30"&gt; &lt;GlyphOrder&gt; &lt;!-- The 'id' attribute is only for humans; it is ignored when parsed. --&gt; &lt;GlyphID id="0" name="glyph00000"/&gt; &lt;GlyphID id="1" name="x"/&gt; &lt;GlyphID id="2" name="8"/&gt; &lt;GlyphID id="3" name="4"/&gt; &lt;GlyphID id="4" name="9"/&gt; &lt;GlyphID id="5" name="0"/&gt; &lt;GlyphID id="6" name="2"/&gt; &lt;GlyphID id="7" name="5"/&gt; &lt;GlyphID id="8" name="7"/&gt; &lt;GlyphID id="9" name="1"/&gt; &lt;GlyphID id="10" name="_"/&gt; &lt;GlyphID id="11" name="_#1"/&gt; &lt;GlyphID id="12" name="_#2"/&gt; ...]]></content>
      <tags>
        <tag>字体</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django的学习记录]]></title>
    <url>%2Farticle%2F3399%2F</url>
    <content type="text"><![CDATA[Django的一些基础知识，例如Django框架的基本结构(1)为什么要选择Django 简介：Django是使用Python语言开发的一套免费开源的后台Web框架，主要为前端Html提供后台数据支持，后台项目的运行需要托管于服务器，从而接收前端发生的请求，后台予以响应。1.一站式的解决方案（ORM，Session，Admin等）可以做后台的定时任务，执行相关的任务；也可以做运维的管理后台，可以快速的搭建起一个后台的模型Django的ORM是值得肯定的。因为有了它的ORM模型，我们可以方便的调用modles，直接去操作数据库，这样省去了手写SQL，或者SQL安全方面的一些问题2.成熟的Python Web框架（Django社区、丰富模块、稳定）相对于现在主流的其他几款Python Web框架而言，Django开源于05年07月，所以相对于现在来说比较早，就因为这一点，他有着丰富的Django社区。对于出现的问题可以快速的查找到、定位到相应的问题，找到解决方案，对于新手而言，也可以快速的上手，进行学习。对于新手主要要去浏览Django的官方文档 官方文档：https://www.djangoproject.com/，里面有Django的整套不同版本的文档，主要是英文的。但是文档里从基础一直讲到相当细节的内容。对于开发者而言是相当方便的。当然了要想看中文的可以去百度搜索Django中文文档。模块：有了丰富的模块，可以开发出一些多样的内容，省去了需要自己从零开发代码和相关的一些底层的设计所需要的耗时。(2)Django工程结构和建立首先安装好Django后，通过Django-admin startproject 项目名新建一个项目。应用的建立(scanhosts：应用名)Ubuntu: ./manage.py startapp scanhostsWindows: python manage.py startapp scanhostsDjango目录结构下面是Django的目录的大体结构：首先最外层的是容器项目名–注意：这个容器项目，没有什么实际的作用，修改容器项目的名字，并不会影响后面项目的运行。manage.py：是一个命令行工具，会接收对于自己的Django工程的管理，比如Django工程的启动和关闭，或者进行相应的调试，或者是数据模型的迁移等等操作，都是用这个文件进行操作。–里面的内容很简单就是从终端接收对应的参数去执行对应的命令，所以起到的是一个Django的管理工具的作用。setting.py：详细配置非常重要！这个里面有Django工程里面初始化的所有的相应的默认设置，也有后面我们要对Django工程对应的核心模块的一些设置都会集中在settings这个模块里面。Django初始化启动的服务也都是通过settings里面对应的配置去读取urls.py: 用于做Django工程的url路由的——我们要做自己的Web后台服务的时候，要生成自己的http的url，urls.py就是url的路径进行对应的路由的。wsgi.py：这个是Django自己的一个与wsgi兼容Web服务的接口。其他文件夹：然后下面就是包含的应用项目，一个容器项目里面可以包含多个应用项目scanhosts：自己建立的应用models.py：重点了解 Django的ORM模型建立model这个模型的一些配置都会放到这里面,默认里面只有下面这一行代码。这里是指导入了Django的模型，我们在定义数据库模型的时候，就要在这个文件里建立。1 2 from django.db import models# Create your models here.migrations文件夹：默认是空的，这里是要做数据迁移用的。我们在models这个文件里进行对应的数据库模型的建立和更改以后，为了进行一个对应的迁移操作，会生成一些临时文件（一些临时的但是非常重要的中间文件）在migrations里面。会保存对应的文件的操作信息在这个文件夹里面。views.py： 是一个视图文件，同样也是在我们如果需要进行Web后台或者接口服务的话，对应相应的逻辑处理的逻辑层的代码书写都会放在这个文件里。(后台项目的主要逻辑就是在这个文件中，主要是给url绑定响应的数据处理函数，并返回响应对象。)admin.py： 用于配置Django框架提供的一套后台管理页面的文件。apps.py: 主要是对于APP应用进行配置的文件。后台项目和app应用的关系:类似于爬虫项目和Spider之间的关系，一个后台项目可以管理多个应用，每一个app应用分别对应着这个项目的不同模块。一个项目分为：登录注册、商品信息、用户中心、购买这个四个模块，在同一个项目中创建4个APP应用分别对应这个4个模块，每人负责两个模块。(3)第一个DevOPS工程启动Django工程的命令python manage.py sunserversettings文件详细配置1 2 3 # Build paths inside the project like this: os.path.join(BASE_DIR, ...)# BASE_DIR 是一个路径，工程启动的初始化基础路径，后面会调用基础路径去取其他的文件BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))1 2 3 4 5 6 7 8 9 10 11 12 13 14 # 新建的应用如果没有加到settings文件里是不会随着Django工程一起启动的。所以首先要做的就是把自己的项目添加到INSTALLED_APPS这个列表里，最好直接添加到最后面。# Application definitionINSTALLED_APPS = [ # 前面都是Django的一起默认的基础模块 'django.contrib.admin', # 后台模块 'django.contrib.auth', # 认证相关的模块 'django.contrib.contenttypes', 'django.contrib.sessions', 'django.contrib.messages', 'django.contrib.staticfiles', 'scanhosts',]1 2 3 4 5 6 7 8 9 10 # 数据库的连接，也是需要配置的。默认使用的是sqlite的数据库# Database# https://docs.djangoproject.com/en/2.1/ref/settings/#databasesDATABASES = &#123; 'default': &#123; 'ENGINE': 'django.db.backends.sqlite3', 'NAME': os.path.join(BASE_DIR, 'db.sqlite3'), &#125;&#125;(4)Django日志logging模块(5)Django邮件发送]]></content>
      <categories>
        <category>Django</category>
      </categories>
      <tags>
        <tag>Django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[scrapy部署爬虫项目]]></title>
    <url>%2Farticle%2F38441%2F</url>
    <content type="text"><![CDATA[功能：它就相当于是一个服务器，用于将自己本地的爬虫代码，打包上传到服务器上，让这个爬虫在服务器上运行，可以实现对爬虫的远程管理。(远程启动爬虫，远程关闭爬虫，远程查看爬虫的一些日志。)1.scrapy的安装pip install scrapy2.如何将本地的爬虫项目Deloying(打包)，上传至scrapyd这个服务中。a&gt; 提供了一个客户端工具，就是scrapyd-client，使用这个工具对scrapyd这个服务进行操作，比如向scrapy这个服务打包上传项目。scrapy-client类似于redis-cli.exe,mongodb数据库的client。pip install scrapyd-client==1.2.0a13.上述服务和客服端安装好之后，就可以启动scrapyd这个服务了。服务启动之后，不要关闭，访问 http://127.0.0.1:6800/出现下面的页面正常4.配置爬虫项目，完成以后，再通过addversion.json进行打包。5.上述的scrapyd服务窗口cmd不要关闭，再心打开一个cmd窗口，用于使用scrapyd-client客户端连接scrapyd服务。进入项目根目录，然后输入scrapyd-delopy命令，查看scrapyd-client客户端命令是否正常可用1 2 输入：scrapyd-deploy返回：Unknown target: default查看当前可用于打包上传的爬虫项目：1 2 输入：scrapyd-deploy -l返回：bole(scrapy.cfg里添加的爬虫名称) http://localhost:6800/(scrapy.cfg里解注释的url)使用scrapy-deploy命令打包项目：scrapyd-deploy bole -p jobbolespider参数：Status:”ok”/“error” 项目上传状态Project: 上传的项目名称Version: 项目的版本号，值是时间戳Spiders: 项目project包含的爬虫个数通过API接口，查看已经上传至scrapyd服务项目。1 命令：curl http://localhost:6800/listprojects.json`键值：Projects:[]所有已经上传的爬虫项目，都会显示在这个列表中。通过API接口，查看某一个项目中的所有的爬虫名称；1 命令： $ curl http://localhost:6800/listspiders.json?project=myproject通过API接口，启动爬虫项目：1 命令： curl http://localhost:6800/schedule.json -d project=爬虫项目名称名称 -d spider=项目中某一个爬虫名称键值：jobid：是根据项目(jobbolespider)和爬虫(bole)生成的一个id，将来用于取消爬虫任务。1 取消：curl http://localhost6800/cancel.json -d project=jobbolespider -d job=**1 删除命令：$ curl http://localhost:6800/delproject.json -d project=myproject注意：如果项目上传失败，需要先将爬虫项目中打包生成的文件删除(build、project.egg-info、setup.py)注意：删除前一定要先取消！]]></content>
      <categories>
        <category>scrapy</category>
      </categories>
      <tags>
        <tag>部署</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式爬虫]]></title>
    <url>%2Farticle%2F19872%2F</url>
    <content type="text"><![CDATA[1. 概念将同一个爬虫程序放在多台电脑上(或者同一个电脑中的多个虚拟机环境)，并且在多台电脑上同时启动这个爬虫。异态电脑运行一个爬虫程序成为单机爬虫。2. 作用可以利用多台电脑的带宽，处理器等资源提高爬虫的爬取速度；3. 原理分布式需要解决的问题（共用的队列Queue和共用的去重集合set(),这两个是scrapy自身无法解决的问题。只能通过第三方组件。）需要保证多台电脑中的爬虫代码完全一致；多台电脑操作统一网站，如何管理url去重？scrapy如何做去重的？scrapy会将所有的Request对象(request.url,request.method,request.body)进行加密，生成一个指纹对象fingerprint；然后将指纹对象放入set()集合中进行对比，如果set()集合中已经存在这个对象，那么scrapy就会将Request抛弃。如果set()集合中不存在这个指纹对象，就将这个Request添加到调度队列queue中，等待被调度器调度；多台电脑就会有多个set()对象。能否使用各自的set()做去重？不能。这个set()对象中的数据都是保存在电脑内存中的，电脑的内存空间是不能共享的。这个set()对象随着程序的启动而创建，程序的退出而销毁所以，要解决这个问题需要让所有电脑公用同一个set()集合，不再使用scrapy内置的set()对象，而是使用scrapy-redis蒋政set()集合在redis中创建。然后多台电脑访问同一个redis就可以实现set集合的共用。用于存放合法Request请求对象的对象，在scrapy中也是默认存放在内存中的，也是无法实现多台电脑的共享队列。如果是多台电脑的话，就需要保证多台电脑有共用的队列queue，这样可以保证所有电脑都是从同一队列中获取Request对象进行调度，多有set()过滤出来的Request都放到同一队列中。所以，要解决这个问题，需要让所有电脑公用同一个队列，不再使用scrapy内置的queue，而是使用scrapy-redis将这个queue在redis中创建，然后多台电脑访问同一个redis就可以使用queue的共用。如何将不同电脑上获取的数据，保存在同一个数据库中。4. 配置必须配置1 2 3 4 # Enables scheduling storing requests queue in redis.SCHEDULER = "scrapy_redis.scheduler.Scheduler"# Ensure all spiders share same duplicates filter through redis.DUPEFILTER_CLASS = "scrapy_redis.dupefilter.RFPDupeFilter"可选配置1 2 3 4 # Store scraped item in redis for post-processing.ITEM_PIPELINES = &#123; 'scrapy_redis.pipelines.RedisPipeline': 300&#125;必须配置1 2 3 4 5 6 7 8 # 配置REDIS_URL = '(redis://redis数据库用户(默认是root)：redis数据库连接密码(默认为空))@(redis的连接地址):(redis的端口号)'# Specify the full Redis URL for connecting (optional).# If set, this takes precedence over the REDIS_HOST and REDIS_PORT settings.# hostname：redis的链接地址，由于多台电脑要连接同一个redis数据库，所以，这个链接地址可以是其中一台电脑的IP地址。注意：这个IP地址对应的电脑必须启动redis，# 如果是阿里云上购买的redis数据库服务器，这个hostname就填写阿里云的redis服务器的公网IP#REDIS_URL = 'redis://root@ip:6379'#REDIS_HOST = 'localhost'#REDIS_PORT = 6379必须配置配置redis服务，默认只支持localhost的本地链接，不允许远程链接，需要配置redis服务，开启远程连接功能。找到本地安装包，修改redis.windows.conf文件将protected-mode yes 这个保护模式改为 no将bind 127.0.0.1 修改成REDIS_URL中设置的IP地址(**)注意：分布式爬虫程序中设置的所有的key，都是”xxx:xxx”结构，对应的是容器的名称（类似于MySQL中的表名），并不是容器内容中key：value的这个key。> (必须配置) from ..scrapy_redis.spiders import RedisSpider class BoleSpider(RedisSpider): name = 'bole' allowed_domains = ['jobbole.com'] # start_urls = ['http://blog.jobbole.com/all-posts/'] # 分布式就不需要再设置起始的url，需要通过redis进行添加起始的url。起始的url添加到哪去了？被添加到了公共队列queue中。让多台机器中的其中一台从公共的reids队列queue中，获取起始的url，并对这个起始的url进行请求和解析，获取更多的url，然后将所有的url构造成Request，还放入公共队列queue中，让其他机器获取这些Request请求。 redis_key = 'jobbole:start_urls' > 向redis_key = 'jobbole:start_urls'这个键中，添加起始url. lpush jobbole:start_urls http://blog.jobbole.com/all-posts/ > 修改MySQL数据库为远程连接，让所有电脑连接同一个数据库，爬取出来的数据都保存在同一个数据库的表中。 默认情况下，MySQL分配的root用户只允许本地连接localhost，如果需要通过IP建立数据库的连接，需要创建一个具有远程连接权限的用户： *.*: 所有连接地址都可以使用，比如：localhost,192.168.1.121,117.56.23.4 rootuser: 新创建的用户名 '%': 表示所有权限 '123456': 连接密码 grant all privileges on *.* to rootuser@'%' identified by '123456';(需要从cmd先进入mysql，再输入这个命令) ### scrapy去重：start_url = [‘http://blog.jobbole.com/all-posts/&#39;, ‘http://blog.jobbole.com/all-posts/&#39;] 这个列表中的所有的url都不会去重。1 2 3 4 # dont_filter的默认值是Falsedef start_request(): for start_requests: yield Request(dont_filter=True)1 2 3 4 5 6 7 # 开始去重def enqueue_request(): # 此函数位于core/scheduler文件中。 if not dont_filter and self.request_fingerprint(request): return False else: # 将request添加到队列中除了start_urls之外的这些请求都是要去重的。1 # scrapy-redis去重：和scrapy默认的去重一致。]]></content>
      <categories>
        <category>scrapy</category>
      </categories>
      <tags>
        <tag>scrapy</tag>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[youxinSpider]]></title>
    <url>%2Farticle%2F21872%2F</url>
    <content type="text"><![CDATA[关于某信二手车的验证码问题。这个页面的请求其实不多，也没遇见什么加密 所以几乎是一瞬间就发现了请求的网址 -&gt; validate，还顺便看到了它的两个参数vcode：后面跟的是四位验证码t：后面跟的是网页中图片验证码的链接里面的于是乎，乐呼呼的填上了，但是直接请求失败。403状态码。还是接着研究吧。scrapy采用跟的是多线程模式，十个线程，每一个都遇到了验证码，所以十个线程肯定都会去刷新验证码图片，要是能保证最新的验证码给服务器进行认证也行，但是没办法获取到最新的啊。##]]></content>
      <categories>
        <category>scrapy</category>
      </categories>
      <tags>
        <tag>scrapy</tag>
        <tag>Spider</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于此博客图床的使用]]></title>
    <url>%2Farticle%2F28415%2F</url>
    <content type="text"><![CDATA[就像很多文章中写的那样，要是后期需要在微博中配大量的图片无疑会大大降低网页的加载速度，本来把网页托管在github pages中链接速度就很不给力，不能再被图片拖了后腿，试过七牛云的对象存储功能，确实功能很强大、很全面。但是..要备案，用图床本来也不是刚需，加上主要原因：自己也买不起阿里的服务器。。就暂时搁置了看了其他网站基本上也都差不多的，因为国内的都需要备案嘛，国外的速度又不快。所以就自然而然的盯上了微博图床。好废话不多说，测试阶段好像也不需要，那就直接写怎么用吧，我这里用的是一款浏览器插件就叫微博图床，别人在github上的开源项目@https://github.com/Semibold/Weibo-Picture-Store 这里放上链接，有兴趣的可以去看看。使用非常简单，只需要在浏览器中登录过新浪就行了，这个插件就不需要单独配置了，直接就能上传图片，生成外链，好了废话不多说，下面的一张图就当是测试了。还在考虑到底要不要在这里放上背景图片，因为背景图用的是@https://unsplash.com/ 这个网站的提供的接口，可以自定义相册什么的，使用非常方面，最重要的是竟然支持自定义尺寸，它会给你在服务端裁剪好。唯一的缺点就是。。在国内访问也太慢了。但是转到微博图床后，就只能使用那几个图片，而且还要手动修改尺寸，还要考虑怎么随机获得这个图片。一大堆麻烦事，开始尝试一下吧。好了，到此结束。其实没写什么东西，就是顺便练习练习怎么写博客，以后留给自己看吧！]]></content>
      <tags>
        <tag>图床</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[新浪微博爬虫]]></title>
    <url>%2Farticle%2F23444%2F</url>
    <content type="text"><![CDATA[爬取新浪微博时遇到的一些小问题汇总，但是由于现在还没有养成，报错就截图的好习惯，所以现在只能捡能想起来的先写点。先说一下一开始遇到的这个错误，经过查找原因，发现可能是由的网址的拼接规则不对1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 ValueError: Missing scheme in request url: 2018-09-16 17:19:40 [scrapy.core.scraper] ERROR: Spider error processing &lt;GET https://weibo.cn/search/mblog?hideSearchFrame=&amp;keyword=%E8%B5%B5%E4%B8%BD%E9%A2%96&amp;page=2&gt; (referer: None)Traceback (most recent call last): File "F:\ENVS\scrapy_two\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback yield next(it) File "F:\ENVS\scrapy_two\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output for x in result: File "F:\ENVS\scrapy_two\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in &lt;genexpr&gt; return (_set_referer(r) for r in result or ()) File "F:\ENVS\scrapy_two\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in &lt;genexpr&gt; return (r for r in result or () if _filter(r)) File "F:\ENVS\scrapy_two\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in &lt;genexpr&gt; return (r for r in result or () if _filter(r)) File "E:\yuke\test\9-13\XinLangSpider\XinLangSpider\spiders\weibo.py", line 18, in parse yield scrapy.Request(url=url,callback=self.parse_info) File "F:\ENVS\scrapy_two\lib\site-packages\scrapy\http\request\__init__.py", line 25, in __init__ self._set_url(url) File "F:\ENVS\scrapy_two\lib\site-packages\scrapy\http\request\__init__.py", line 62, in _set_url raise ValueError('Missing scheme in request url: %s' % self._url)发现一个可能是错误的原因的地方修改之前1 2 3 for every_article in page: url = every_article.xpath('div/a[@class="cc"]/@href').extract_first('') yield scrapy.Request(url=url,callback=self.parse_info)修改之后1 2 3 url_list = page.xpath('div/a[@class="cc"]/@href').extract()for url in url_list: yield scrapy.Request(url=url,callback=self.parse_info)其实也就是等于以前在for循环里整理网址,修改后在for循环之外整理网址,然后遍历网址的列表.猜测可能是因为scrapy爬虫框架要求的高效率吧,这样处理会让爬虫可以同步进行爬取列表中的网址.关于转发数、赞数和评论数的提取，下面是item里面的一部分定义函数，使数量存在时返回，不存在时返回’0’1 2 3 4 5 6 7 8 9 10 11 12 def process_zhuanfa(value): if value[3:-1] is None or value[3:-1] == '': return '0' else: return value[3:-1]def process_zan(value): return value[2:-1]def process_pinglun(value): if value[4:-2] is None or value[4:-2] == '': return '0' else: return value[4:-2]关于cookie池的问题采用的是别人写好的开源的一个项目，输入账号密码后，自动生成cookie，并通过@http://127.0.0.1:5000/weibo/random 来提取生成生成的值获取到的结果为str，需要反序列化成字典request.cookies = requests.get(&apos;http://127.0.0.1:5000/weibo/random&apos;).json() 关于发布时间转成统一格式的问题现在发现有三种格式的时间**分钟之前(一小时之内的会这样显示)今天 :(超过一小时就会这样显示)09月13日 22:33 (不是今天的)现在考虑的是,分别格式化为时间戳,然后再从时间戳转换成要求的统一格式1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 import timedef process_time(value): value = value.split(' ')[0:2] date = 'no_time' if '分钟' in value[0]: time_stamp = time.time() - int(value[0].split('分钟')[0]) * 60 date = time.strftime("%Y-%m-%d %H:%M:%S", time.localtime(time_stamp)) return date if '今天' in value[0]: local_time = time.localtime() time_string = "%s年%s月%s日 %s"%(local_time.tm_year,local_time.tm_mon,local_time.tm_mday,value[1]) date = time.strftime('%Y-%m-%d %H:%M:%S', time.strptime(time_string, '%Y年%m月%d日 %H:%M')) return date if '月' in value[0]: local_time = time.localtime() time_string = "%s年%s %s" % (local_time.tm_year, value[0], value[1]) date = time.strftime('%Y-%m-%d %H:%M:%S', time.strptime(time_string, '%Y年%m月%d日 %H:%M')) return date最后的最后，准备接入微博的图床。也就是用人家的相册做些事情。。下面是测试用的图片还是不要了，现在已经改完了。。]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>scrapy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[zhihu的一些评论]]></title>
    <url>%2Farticle%2F7459%2F</url>
    <content type="text"><![CDATA[代理IP:1.免费的代理IP,可用IP少,时效性较短,大部分的代理IP可能在访问网站都是失效的.2.收费的代理IP,讯代理.3.爬取国外的网站,VPN代理服务器.proxy_pool:@项目地址：https://github.com/jhao104/proxy_pool.git1.它将国内的代理IP网站都进行了爬取;2.代理IP爬取完毕之后,会进行检测,可用的IP会保存到数据库redis中;3.会定期将数据库中的代理IP拿出来检测,失效的IP从数据库中删除。4.支持扩展练习项目:1. 解决登陆问题； 2. 使用downloadmiddleware, ua, 代理、池; 3. 使用itemloader完成数据的提取和解析； 4. 提取所有问题的信息：标题、点赞数、评论数、关注者数量、浏览数量；将列表页数据单独 保存至一个表中； 5. 提取每一个问题的所有答案：回答用户、回答内容、回答时间、回答 点赞数、回答评论数；将所有的答案保存至另外一个表中； 6. 数据库的写入要求使用异步Twisted框架完成； 首先需要先设置好代理IP和请求头,也就是自定义下载中间件然后根据需求知道要先获得登录后的Cookie这个设置在middlewares里cookie登录需要设置成键值对的方式1 2 3 4 5 6 7 8 9 10 11 12 13 14 request.cookies = &#123; '_xsrf':'BKORzjqUj35ax0DdrwKnVn25RkNRaahM', '_zap':'4dc36192-e267-44d1-b8f4-074822456258', 'd_c0':"AFAlC-xzMw6PTlUrgV6b_Bu4DtkilyXo-Z4=|1536751108", 'capsion_ticket':"2|1:0|10:1536756874|14:capsion_ticket|44:MDRlODI3NDk5ZjhlNGRlOThiNzdiN2Q1NGM3OWEyNjQ=|7b1b74e68e3ccc4440577f394f8e1ded1efc6a5bca537a1a0d459b71d36877c2", 'q_c1':'cd8e72dfa9384e70af51bc5c90b10f8a|1536751149000|1536751149000', 'l_n_c':'1', 'l_cap_id':"NGRjMmM4OTZkOTBlNGQ0ZTg2MTQ2YTk4YTNiODBhMGI=|1536756854|ae8c593677b5d5d7301941fa6e133c0c4f4f1022", 'r_cap_id':"YTM4YTAyZjQ2ZDAwNDhiNjlmYTYxNmFmODU2NTMwMWU=|1536756854|b164129500acaa359cfc585c4c1d5d32b8529bf2", 'cap_id':"MjQ0ZWZhOGQwZDlhNGQ5MGE0ZWRlMTNlMWYyZDcwMWQ=|1536756854|a8f854ab2430dbb1dc0f0ee8b9cf89e9f316b556", 'n_c':'1', 'z_c0':"2|1:0|10:1536756886|4:z_c0|92:Mi4xU2JyYkF3QUFBQUFBVUNVTDdITXpEaVlBQUFCZ0FsVk5sbHFHWEFEMGZOMm5qMmItXzJlUElVWEpCWHZGWkFzMkt3|8e0843029fb42283f58a63ec8765ba96c10b936e8775f14293cf3b12501e0f13", 'tgw_l7_route':'53d8274aa4a304c1aeff9b999b2aaa0a' &#125;登录成功之后,获取问题详情,1.发现获取详情的网址是xhr请求,其中一条为https://www.zhihu.com/api/v3/feed/topstory?action_feed=True&amp;limit=7&amp;session_token=b36c08db4577ef8d563788f5337b283c&amp;action=down&amp;after_id=13&amp;desktop=true2.经解析参数发现session_tokon这个参数,每次启动项目都会不一样,所以尝试在主页面的response中寻找.最终解析该值得代码为:1 2 3 4 data = response.xpath('//div[@id="data"]/@data-state').extract_first('')pattern = re.compile(r'sessionToken":"(.*?)"',re.S)session_token = re.findall(pattern,data)print(session_token)3.参数after_id的值为每一次加7,所以设置range循环,步进为7yield回调解析json数据的函数.经测试发现有的问题,没有question的值.–这个问题是因为:有的是文章,有的是问题.–这里暂时只解析那些是问题的页面.网址都是跟据id拼接出来的然后发现https://www.zhihu.com/question/41435689/answer/465837326拼接出这样的网址是无法获取到全部的回答的.所以..还是考虑把后面的回答者去掉变成https://www.zhihu.com/question/41435689这样解析问题页面的评论1.https://www.zhihu.com/api/v4/questions/41435689/answers?发现xhr请求中类似这样的get请求是获取的评论.2.进去后经简单解析,发现没个请求有四个,所以就用从0开始步进为5的循环.–以后可以考虑获取一下评论的总数,然后按总数获取.这一点参数里有两个值offset=5&amp;limit=5这两个控制每次获取的评论数.暂时不需要更改3.include=data%5B*%5D这个参数,好像没个问题的也都不一样.关于这一点,参数里面需要这些东西的都是用[]代替的,写出[]也能正常获取评论4.scrapy获取的json数据我用的解析方式,这样就能获取到字典了1 2 response_data = response.body.decode('utf-8')json_data = json.loads(str(response_data))5.关于评论帖子的时间这一块,返回的好像是个时间戳,需要反序列化一下1 2 3 4 5 6 7 8 9 import timeif 'updated_time' in data: # 有更新时间就获取更新时间 date = data.get('updated_time','')else: # 没有就获取创建时间 date = data.get('created_time','')timeArray = time.localtime(date)date = time.strftime("%Y-%m-%d %H:%M:%S", timeArray)关于遇到的一些错误1 MySQLdb TypeError: %d format: a number is required, not str问题解决这个其实就是传参数的时候占位符填错了,一般都是因为传int类型时不需要用%d,直接用%s就搞定了,亲测成功~!还有一点就是最好要把str类型的占位符改成’%s’,就是带引号的这种,防止1146错误..]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>代理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[用twisted异步写入]]></title>
    <url>%2Farticle%2F16883%2F</url>
    <content type="text"><![CDATA[异步写入MySQL数据库,如果scrapy解析数据的速度远远超过数据库的写入速度,那么很容易就造成数据的丢失.这里用twisted保证数据的异步多线程写入,提高写入速度.详情请看正文@本文相关源码: https://github.com/Loveyueliang/yuke/tree/master/9-11/jobbolespider首先需要先导入模块1 from twisted.enterprise import adbapi接着在自定义的pipeline里面重构from_settings函数1 2 3 4 5 @classmethoddef from_settings(cls, settings): """ 这个函数名称是固定的，当爬虫启动的时候，scrapy会自动调用这些函数，加载配置数据。 """创建一个数据库连接池对象，这个连接池中可以包含多个connect链接对象。1 2 3 # 参数1：操作数据库的包名# 参数2：链接数据库的参数db_connect_pool = adbapi.ConnectionPool('pymysql', **params)初始化这个类的对象1 obj = cls(db_connect_pool)在process_item这个函数里执行数据的多线程写入操作1 2 3 4 5 # 参数1：在线程中被执行的sql语句# 参数2：要保存的数据result = self.dbpool.runInteraction(self.insert, item)# 给result绑定一个回调函数，用于监听错误信息result.addErrback(self.error)定义一个打印错误信息的函数1 2 def error(self, reason): print('--------', reason)定义一个执行插入语句的函数,用twisted的好处其中一点就是不用一步步的手动提交1 2 3 4 def insert(self, cursor, item): insert_sql = "INSERT INTO bole(bole_title, bole_date, bole_tag, bole_content, bole_dz, bole_sc, bole_pl, bole_img_src) VALUES (%s, %s, %s, %s, %s, %s, %s, %s)" cursor.execute(insert_sql, (item['bole_title'], item['bole_date'], item['bole_tag'], item['bole_content'], item['bole_dz'], item['bole_sc'], item['bole_pl'], item['bole_img_path'])) # 不需要commit()]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>scrapy</tag>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于51job的爬虫说明]]></title>
    <url>%2Farticle%2F24762%2F</url>
    <content type="text"><![CDATA[@本文源码：https://github.com/Loveyueliang/yuke/tree/master/9-10错误点:感觉唯一的坑就是刚学的sql插入,一开始定义的表的字段名和item定义的不一样,所以就各种报KeyError.1 2 insert_sql = &quot;INSERT INTO job(zwmc, zwxz, gzdd, gzyq)VALUE (&apos;%s&apos;, &apos;%s&apos;, &apos;%s&apos;, &apos;%s&apos;)&quot;%(item[&apos;zwmc&apos;],item[&apos;zwxz&apos;],item[&apos;gzdd&apos;],item[&apos;gzyq&apos;]) self.cursor.execute(insert_sql)最终结果还不错,暂时就爬取这一点信息吧.]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>scrapy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[xieyuying]]></title>
    <url>%2Farticle%2F61139%2F</url>
    <content type="text"><![CDATA[@本文源码：https://github.com/Loveyueliang/XieYuYing.gitsi: 这里面写一写个人需求或建议的一写东西.调用剧情,选择用1-11这种类似的东西关于人物类的创建,人物可以考虑做在一个文件里,共同继承一个父类People –现在这一项已经完成.关于玩家名字的说明: 调用更改玩家名字的时候, 直接用赋值语句,现有判定输入只能为中文名字关于好感度的说明:好感度设计的逻辑一开始没有理清.现已完成1.0版本–定义了一个好感度的类,方法为增加/减少好感度,目前是女主和胖老板的父类.理想参数为执行的相应动作,返回值为增加/减少的好感度.但是目前还没有定义需要的所需要的以动作和数字为键值对的字典.动作多的话倒是可以采用这种方法, 要是很少的几个动作,估计要重新给每一个动作编写一个方法好感度采用等差数列递增的方式.多引用一个增长速度2.0最新进度,可以采用函数右移的方法来实现偶尔的坏话处理,,把函数做成列表生成器,达到每调用一次就可以得到下一个结果生成器也做好了,但是需要一个初始的好感度,也就是说,,先给每个人用生成器生成到相应想给到的好感度3.0最终版本:终于搞定了好感度最终定义为一个列表增加好感度的方法里面先查询现在好感度的索引,然后通过索引加1的方法调用下一个应该得到的好感度.关于商品的说明: 现在还没有开始做不过估计要创建一个商品的类,方法的话,应该就只有购买,不过属性可能会有点多.需要注意: 要添加的一个功能–通过购买可以选择删除剧情中的某个人物,感觉关联度有点高]]></content>
      <categories>
        <category>类的使用</category>
      </categories>
      <tags>
        <tag>面向对象</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机开机]]></title>
    <url>%2Farticle%2F33956%2F</url>
    <content type="text"><![CDATA[计算机启动过程BIOS简介:基本的输入输出系统,连接底层的硬件和系统第一步: 按下电源键按下电源键的时候电源开始向主板和cpu供电,一开始电压不稳定,主板控制芯片会一直向cpu发送reset信号,让cpu初始化一般这个过程非常短暂,几乎在一瞬间电压就会稳定,控制芯片撤掉reset信号.但按下reset的时候,这个过程就特别明显….第二步: BIOS硬件自检–检测关键性设备BIOS开始上电自检,这个地方会主要会检测内存,显卡,要是检测不通过,蜂鸣器会报告检测到的错误,一般用滴的长短和次数来表示错误的类型这个地方检测的内存地址只有640k.第三步: 屏幕亮,检测显卡BOIS这里系统BIOS会读取显卡的BIOS启动显卡,此时屏幕开始亮起,会简单的显示显卡的一些参数,要是不亮,就…第四步: 显示BIOS自己的界面会显示BIOS日期,主板芯片组型号,主板识别码,主板厂商,等一堆东西.第五步: 仔细检测cpu和内存会把检测到的cpu类型,主频打印在屏幕上下面会跟着开始检测所有内存信息第六步: 检测标准硬件设备检测硬盘 cd-rom 软驱 并行接口,和串行接口第七步: 检测外围设备检测那些可以即插即用的东西,比如usb接口的U盘什么的.检测到后为每一个检测到的设备分配dma.i/o接口等每检测到一个设备,都会把信息打印在屏幕上第八步: BIOS清屏,重新显示系统配置表会在系统上方显示一个概括性的配置列表,列出了系统安装的各种各种标准硬件设备以及他们使用的资源和相关参数.第九步: 检测硬件更新这里的硬件指的是类似外接内存,声卡,网卡,显卡,等设备有没有被更换,如果更换了,就更新一下存储,方面系统读取配置.第十步: 根据启动顺序,启动引导这一步,才开始引导操作系统,系统BIOS读取并执行硬盘的主引导记录,从分区表中找到第一个活动分区,然后读取并执行分区的引导记录.分区的引导记录负责读取并执行系统下面是结构图]]></content>
      <categories>
        <category>前端</category>
      </categories>
      <tags>
        <tag>capture</tag>
      </tags>
  </entry>
</search>
