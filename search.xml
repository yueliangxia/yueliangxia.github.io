<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[关于新手购买域名和选择服务器的说明]]></title>
    <url>%2F2018%2F10%2F11%2Fabout-dn-server%2F</url>
    <content type="text"><![CDATA[最近身边的好多小伙伴都购买了域名，但是不知道该怎么部署才能实现装逼效果，下面我就简单的写一点相关的说明和我遇到的一些坑。写的不好请见谅。 必读 写在前面本文的内容是购买域名和相对应域名商的解析设置，然后搭建的过程会贴出大佬的文章链接。 首先当然是考虑先买一个域名啦。服务器什么的慌什么，也跑不掉。现在好多的服务商都提供域名购买服务，其实买到的域名都是一样的，就是各大服务商的政策有一点区别。 1.首先各大服务商基本都有域名购买服务。比如阿里云，腾讯云，百度云···等 2.这里说一下购买步骤。 在云服务商主页，这里采用的是腾讯云的例子。一般找不到的话上面都有搜索，输入域名。如下图 后面的步骤这里就不演示了，花钱买就行了。不过要说一下，要是第一个域名的话，腾讯云的会有一个活动，不是的话就算了。 服务器提供商和博客框架的选择域名购买之后就是要选择服务器提供商了。虽然说国内各大网站卖的肯定是各种服务器都有，但是1.贵，2.都需要备案。所有暂时都不考虑 服务器：这里只说一下我比较了解的两个没有以上两点问题的服务器：1.GitHub Pages的部署相关; 2.Nc主机的部署相关 博客框架：这里也只说两个，一个是无后台的Hexo，一个是有后台管理界面的WordPress。 选择前须知：要是喜欢部署时稍微简单点的，建议先选择nc主机，然后搭配WordPress框架，但其实我感觉难易程度总体来说是差不多的。反正个人也就是我现在用的是GitHub Pages搭配的Hexo框架。理论上说，各个服务器能搭配的框架都是很多的，但还是建议根据上面的搭配来搞。 GitHub Pages的部署相关 这个是由GitHub提供的静态网页服务。官网地址为https://pages.github.com/，有兴趣的可以去瞅瞅。不过人家是全英的。。。 开始搭建博客：在本地安装HEXO，建议阅读一遍官方文档，当然不想看了，就看下面的步骤也行. 安装前需要的必须组件：Git和Node.js。这两个都直接去官网下载安装就行了，这里就不写步骤了，毕竟网上有好多。而且只需要一直下一步就行了。 Node.js安装时一定要注意Custom Setup这一步要选Add to PATH添加到环境变量 开始安装Hexo： 所在目录：~/blog/1 hexo init Nc主机的部署相关 这个主机嘛，提供的有免费虚拟机和的一个免费的大小还凑合数据库。说的是距离近，但是不花钱，访问速度也就比github提供的稍微快一点点。不明显！ 1. 首先去人家的官网http://idc.ncshu.com/会员中心自己注册个账号。 2. 接下来去官网选择免费主机，然后选香港免费主机，点击下图立即订购 3. 配置账户界面 ：这个控制面板密码将会是你的ftp访问和数据库访问的密码。建议浏览器自动保存一下，不然忘了又麻烦。这里建议先写上域名不然的话就自己去控制面板选择绑定域名。 4. 控制面板：搞好之后进入会员中心，然后选择已购产品-主机管理–然后进入管理面板 5. 上传文件： 先去WordPress官网下载这个框架的包。当然，下载地址我贴出来了https://wordpress.org/latest.zip。下载完成后记住地址一会儿把这个框架上传到NC主机 回到NC主机的控制面板–选择FTP/文件管理–在线文件管理器–然后在右侧选择上传文件–选择刚才下载的压缩包文件上传 上传完成后，目录结构如下图，选择解压，解压目录写上/wwwroot,密码写控制面板的密码 6. 回到基本功能–默认文件–文件名写上index.php，顺序写100–提交 7. 绑定域名： 首先按照提示 增加域名绑定，在添加绑定之前请先解析域名：A记录到IP xxxx。这一点请查看下面域名绑定相关，绑定好后进行下一步 域名填写自己买的域名，目录写 wwwroot/wordpress/。—确定 如果没什么问题的话，你刚才填上的域名就能访问了。 访问自己的域名 下面就是正常的WordPress安装界面 7. WordPress的安装： 选择简体中文–然后继续 这里的用户名密码，是你的网站的后台管理员的用户名密码，这个一定要记着。 准备好上面的东西，然后–现在就开始 然后让你输入数据库名，用户名密码什么的。实际上需要输入的也就只有这两个—不知道的话，打开主机控制面板首页–右边那一列有数据库名，数据库用户名 然后选–现在安装，后面的话访问域名，基本的网页就出来了。。如果需要定制，后面再说，或者自己去找相关教程。 域名绑定相关 点击下载百度]]></content>
      <categories>
        <category>域名和服务器</category>
      </categories>
      <tags>
        <tag>IP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[天眼查的字体加密]]></title>
    <url>%2F2018%2F10%2F09%2Ftianyancha%2F</url>
    <content type="text"><![CDATA[关于一些网站的字体加密方面，目前发现的也就只有两种：一种是起点网的那个总字数那种的给的是是禁止的数据，转换成十六进制之后再在woff文件转换成的xml文件中寻找对应的关系。另一种就是类似于天眼查的，获取到的源码是字体的位置信息，然后需要根据位置信息取出对应的数据。 前提： 这里我用了两种方式来搞定这种相对应位置的字体加密。 1.第一种的思路是：主要采用PIL图像处理模块来根据1-0这个十个源码里获得的数字取出字体文件里真正要显示的数字，并画在画布上保存成文件，然后利用图片识别(tesseract-orc)技术来取出保存的图像中包含的数字，从而获得它们的对应关系。 1.1：首先当然是需要安装必须的工具，俗话说“工欲善其事，必先利其器”嘛。 Win系统下首先要安装这个 tesseract-ocr.exe 软件，下载地址就靠自由发挥了。(这个软件需要手动把目录添加到环境变量) 对于Python而言，需要装一个中间模块，用来在py文件中调用上面的软件。（这个好装：pip install pytesseract） 1.2： 2.第二种思路就是还是解析字体转换成的xml文件，从中找到它们之间的对应关系。这次就主要先写这一个吧。代码 我还是直接在代码中注释出来吧。虽然不一定适合其他网站，但是思路还是不错滴。 解析转换成xml的woff文件代码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 from fontTools.ttLib import TTFontfrom xml.etree import ElementTreemap_dict = &#123;&#125;def parse_font_xml(font_url): # 首先 全局定义的有一个空字典，用于存放已经解析过的字体规则。如果已经解析过，则直接取出并返回，提升运算速度。 if font_url in map_dict: return map_dict[font_url] # position_number这个列表定义的是映射前的字符，也就是指源代码中可能获取到的数字。 position_number = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '0'] # 请求字体文件的url，获取字体文件的内容并存储到本地。 font_response = requests.get(font_url, headers=&#123; 'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100 Safari/537.36'&#125;).content with open('tianyan.woff', 'wb') as f: f.write(font_response) # 存好之后，用TTFont打开，并保存成xml文件 f = TTFont('tianyan.woff') f.saveXML('tianyan.xml') # 解析xml标签结构，并取出一个含有所有数字的长度为十的列表命名为maps_element.(最下面有此段的xml代码，但只是众多规则中的一种，仅供参考) root_obj = ElementTree.parse('tianyan.xml').getroot() maps_element = root_obj.find('GlyphOrder').findall('GlyphID')[2:12] # 有的规则会获取不到完整的十个数字，因为有的映射关系没有改变（比如3-3），这样的情况在上面的列表中就不会显示，所以要进行下面的两个步骤 # 1.先遍历maps_element判断取出的值是否为数字，并把是数字的存到新的列表sort_num中。 sort_num = [] for i in range(len(maps_element)): if maps_element[i].attrib['name'] in position_number: sort_num.append(maps_element[i].attrib['name']) # 2.然后判断sort_num这个列表长度是否为十，为十的话，就根本不用处理了 if len(sort_num)!=10: # 如果不为十，就要在映射前的列表中取值，看看映射后的列表也就是sort_num中缺少了什么用insert插入相对应的位置 for num in position_number: if num not in sort_num: sort_num.insert(int(num),num) # 处理完成之后，映射后的数字就按照正确的顺序存在了sort_num这个列表中 # 最后一步就是把值和对应的索引取出来并放在字典中，返回出去 sort_num_dict = &#123;&#125; for key,value in enumerate(sort_num): sort_num_dict[str(value)] = key # 当然不能忘了把字体url和对应的解析规则存放起来，以便下次重复时调用 map_dict[font_url] = sort_num_dict return sort_num_dict 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 &lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;ttFont sfntVersion="\x00\x01\x00\x00" ttLibVersion="3.30"&gt; &lt;GlyphOrder&gt; &lt;!-- The 'id' attribute is only for humans; it is ignored when parsed. --&gt; &lt;GlyphID id="0" name="glyph00000"/&gt; &lt;GlyphID id="1" name="x"/&gt; &lt;GlyphID id="2" name="8"/&gt; &lt;GlyphID id="3" name="4"/&gt; &lt;GlyphID id="4" name="9"/&gt; &lt;GlyphID id="5" name="0"/&gt; &lt;GlyphID id="6" name="2"/&gt; &lt;GlyphID id="7" name="5"/&gt; &lt;GlyphID id="8" name="7"/&gt; &lt;GlyphID id="9" name="1"/&gt; &lt;GlyphID id="10" name="_"/&gt; &lt;GlyphID id="11" name="_#1"/&gt; &lt;GlyphID id="12" name="_#2"/&gt; ...]]></content>
  </entry>
  <entry>
    <title><![CDATA[Django的学习记录]]></title>
    <url>%2F2018%2F09%2F28%2Fdjango-study%2F</url>
    <content type="text"><![CDATA[Django的一些基础知识，例如Django框架的基本结构 (1)为什么要选择Django1.一站式的解决方案（ORM，Session，Admin等） 可以做后台的定时任务，执行相关的任务；也可以做运维的管理后台，可以快速的搭建起一个后台的模型 Django的ORM是值得肯定的。因为有了它的ORM模型，我们可以方便的调用modles，直接去操作数据库，这样省去了手写SQL，或者SQL安全方面的一些问题 2.成熟的Python Web框架（Django社区、丰富模块、稳定） 相对于现在主流的其他几款Python Web框架而言，Django开源于05年，所以相对于现在来说比较早，就因为这一点，他有着丰富的Django社区。对于出现的问题可以快速的查找到、定位到相应的问题，找到解决方案，对于新手而言，也可以快速的上手，进行学习。 对于新手主要要去浏览Django的官方文档 官方文档：https://www.djangoproject.com/，里面有Django的整套不同版本的文档，主要是英文的。但是文档里从基础一直讲到相当细节的内容。对于开发者而言是相当方便的。当然了要想看中文的可以去百度搜索Django中文文档。 模块：有了丰富的模块，可以开发出一些多样的内容，省去了需要自己从零开发代码和相关的一些底层的设计所需要的耗时。 (2)Django工程结构和建立 首先安装好Django后，通过Django-admin startproject 项目名 新建一个项目。 应用的建立(scanhosts：应用名)Ubuntu: ./manage.py startapp scanhostsWindows: python manage.py startapp scanhosts 下面是Django的目录的大体结构： 首先最外层的是容器项目名–注意：这个容器项目，没有什么实际的作用，修改容器项目的名字，并不会影响后面项目的运行。 manage.py：是一个命令行工具，会接收对于自己的Django工程的管理，比如Django工程的启动和关闭，或者进行相应的调试，或者是数据模型的迁移等等操作，都是用这个文件进行操作。–里面的内容很简单就是从终端接收对应的参数去执行对应的命令，所以起到的是一个Django的管理工具的作用。 setting.py：详细配置非常重要！这个里面有Django工程里面初始化的所有的相应的默认设置，也有后面我们要对Django工程对应的核心模块的一些设置都会集中在settings这个模块里面。Django初始化启动的服务也都是通过settings里面对应的配置去读取urls.py: 用于做Django工程的url路由的——我们要做自己的Web后台服务的时候，要生成自己的http的url，urls.py就是url的路径进行对应的路由的。wsgi.py：这个是Django自己的一个与wsgi兼容Web服务的接口。不用做详细的了解其他文件夹：然后下面就是包含的应用项目，一个容器项目里面可以包含多个应用项目scanhosts：自己建立的应用models.py：重点了解 Django的ORM模型建立model这个模型的一些配置都会放到这里面,默认里面只有下面这一行代码。这里是指导入了Django的模型，我们在定义数据库模型的时候，就要在这个文件里建立。1 2 from django.db import models# Create your models here. migrations文件夹：默认是空的，这里是要做数据迁移用的。我们在models这个文件里进行对应的数据库模型的建立和更改以后，为了进行一个对应的迁移操作，会生成一些临时文件（一些临时的但是非常重要的中间文件）在migrations里面。会保存对应的文件的操作信息在这个文件夹里面。views.py： 是一个视图文件，同样也是在我们如果需要进行Web后台或者接口服务的话，对应相应的逻辑处理的逻辑层的代码书写都会放在这个文件里。 (3)第一个DevOPS工程 启动Django工程的命令python manage.py sunserver settings文件详细配置 1 2 3 # Build paths inside the project like this: os.path.join(BASE_DIR, ...)# BASE_DIR 是一个路径，工程启动的初始化基础路径，后面会调用基础路径去取其他的文件BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__))) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # 新建的应用如果没有加到settings文件里是不会随着Django工程一起启动的。所以首先要做的就是把自己的项目添加到INSTALLED_APPS这个列表里，最好直接添加到最后面。# Application definitionINSTALLED_APPS = [ # 前面都是Django的一起默认的基础模块 'django.contrib.admin', # 后台模块 'django.contrib.auth', # 认证相关的模块 'django.contrib.contenttypes', 'django.contrib.sessions', 'django.contrib.messages', 'django.contrib.staticfiles', 'scanhosts',] 1 2 3 4 5 6 7 8 9 10 # 数据库的连接，也是需要配置的。默认使用的是sqlite的数据库# Database# https://docs.djangoproject.com/en/2.1/ref/settings/#databasesDATABASES = &#123; 'default': &#123; 'ENGINE': 'django.db.backends.sqlite3', 'NAME': os.path.join(BASE_DIR, 'db.sqlite3'), &#125;&#125; (4)Django日志logging模块(5)Django邮件发送 极验验证： 抓取极验参数: 任何一个网站，如果在登录时网站接入的极验的接口，那么该网站就可以使用极验验证码进行登录。此时，极验验证码API就会返回两个极验参数(gt和challenge)。这两个参数只跟极验验证码API相关，跟这个网站没有任何关系。 注意： 有的网站是直接调用极验官方提供的验证码接口，比如：极验的官方后台：https://auth.geetest.cn/api/ ；有的网站又对极验验证吗接口封装了一个API接，比如：魅族登录：https://login.flyme.cn/sec/geetest? 将获取的极验参数，提交给极验破解网站的识别接口。会得到新的返回值： 1 2 3 4 5 &#123; status: "ok", challenge: "3d033f099597f5ae63e2e2c902301d183z", validate: "8f6ebd56291ed6569ac40c1d74780985"&#125; 将上述参数challenge换个validate，混合着网站自己的提交的参数向网站自己的url发生POST请求，即可。]]></content>
      <categories>
        <category>Django</category>
      </categories>
      <tags>
        <tag>Django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[scrapy部署爬虫项目]]></title>
    <url>%2F2018%2F09%2F27%2Fscrapy-overview%2F</url>
    <content type="text"><![CDATA[功能：它就相当于是一个服务器，用于将自己本地的爬虫代码，打包上传到服务器上，让这个爬虫在服务器上运行，可以实现对爬虫的远程管理。(远程启动爬虫，远程关闭爬虫，远程查看爬虫的一些日志。) 1.scrapy的安装pip install scrapy 2.如何将本地的爬虫项目Deloying(打包)，上传至scrapyd这个服务中。 a&gt; 提供了一个客户端工具，就是scrapyd-client，使用这个工具对scrapyd这个服务进行操作，比如向scrapy这个服务打包上传项目。scrapy-client类似于redis-cli.exe,mongodb数据库的client。pip install scrapyd-client==1.2.0a1 3.上述服务和客服端安装好之后，就可以启动scrapyd这个服务了。服务启动之后，不要关闭，访问 http://127.0.0.1:6800/ 出现下面的页面正常 4.配置爬虫项目，完成以后，再通过addversion.json进行打包。 5.上述的scrapyd服务窗口cmd不要关闭，再心打开一个cmd窗口，用于使用scrapyd-client客户端连接scrapyd服务。 进入项目根目录，然后输入scrapyd-delopy命令，查看scrapyd-client客户端命令是否正常可用 1 2 输入：scrapyd-deploy返回：Unknown target: default 查看当前可用于打包上传的爬虫项目： 1 2 输入：scrapyd-deploy -l返回：bole(scrapy.cfg里添加的爬虫名称) http://localhost:6800/(scrapy.cfg里解注释的url) 使用scrapy-deploy命令打包项目：scrapyd-deploy bole -p jobbolespider 参数：Status:”ok”/“error” 项目上传状态Project: 上传的项目名称Version: 项目的版本号，值是时间戳Spiders: 项目project包含的爬虫个数 通过API接口，查看已经上传至scrapyd服务项目。1 命令：curl http://localhost:6800/listprojects.json` 键值： Projects:[]所有已经上传的爬虫项目，都会显示在这个列表中。 通过API接口，查看某一个项目中的所有的爬虫名称； 1 命令： $ curl http://localhost:6800/listspiders.json?project=myproject 通过API接口，启动爬虫项目： 1 命令： curl http://localhost:6800/schedule.json -d project=爬虫项目名称名称 -d spider=项目中某一个爬虫名称 键值： jobid：是根据项目(jobbolespider)和爬虫(bole)生成的一个id，将来用于取消爬虫任务。1 取消：curl http://localhost6800/cancel.json -d project=jobbolespider -d job=** 1 删除命令：$ curl http://localhost:6800/delproject.json -d project=myproject 注意：如果项目上传失败，需要先将爬虫项目中打包生成的文件删除(build、project.egg-info、setup.py) 注意：删除前一定要先取消！]]></content>
      <categories>
        <category>scrapy</category>
      </categories>
      <tags>
        <tag>scrapy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式爬虫]]></title>
    <url>%2F2018%2F09%2F25%2Fscrapy-redis%2F</url>
    <content type="text"><![CDATA[1. 概念将同一个爬虫程序放在多台电脑上(或者同一个电脑中的多个虚拟机环境)，并且在多台电脑上同时启动这个爬虫。异态电脑运行一个爬虫程序成为单机爬虫。 2. 作用可以利用多台电脑的带宽，处理器等资源提高爬虫的爬取速度； 3. 原理分布式需要解决的问题（共用的队列Queue和共用的去重集合set(),这两个是scrapy自身无法解决的问题。只能通过第三方组件。） 需要保证多台电脑中的爬虫代码完全一致； 多台电脑操作统一网站，如何管理url去重？ scrapy如何做去重的？scrapy会将所有的Request对象(request.url,request.method,request.body)进行加密，生成一个指纹对象fingerprint；然后将指纹对象放入set()集合中进行对比，如果set()集合中已经存在这个对象，那么scrapy就会将Request抛弃。如果set()集合中不存在这个指纹对象，就将这个Request添加到调度队列queue中，等待被调度器调度； 多台电脑就会有多个set()对象。能否使用各自的set()做去重？ 不能。这个set()对象中的数据都是保存在电脑内存中的，电脑的内存空间是不能共享的。 这个set()对象随着程序的启动而创建，程序的退出而销毁所以，要解决这个问题需要让所有电脑公用同一个set()集合，不再使用scrapy内置的set()对象，而是使用scrapy-redis蒋政set()集合在redis中创建。然后多台电脑访问同一个redis就可以实现set集合的共用。 用于存放合法Request请求对象的对象，在scrapy中也是默认存放在内存中的，也是无法实现多台电脑的共享队列。如果是多台电脑的话，就需要保证多台电脑有共用的队列queue，这样可以保证所有电脑都是从同一队列中获取Request对象进行调度，多有set()过滤出来的Request都放到同一队列中。 所以，要解决这个问题，需要让所有电脑公用同一个队列，不再使用scrapy内置的queue，而是使用scrapy-redis将这个queue在redis中创建，然后多台电脑访问同一个redis就可以使用queue的共用。如何将不同电脑上获取的数据，保存在同一个数据库中。 4. 配置 必须配置1 2 3 4 # Enables scheduling storing requests queue in redis.SCHEDULER = "scrapy_redis.scheduler.Scheduler"# Ensure all spiders share same duplicates filter through redis.DUPEFILTER_CLASS = "scrapy_redis.dupefilter.RFPDupeFilter" 可选配置1 2 3 4 # Store scraped item in redis for post-processing.ITEM_PIPELINES = &#123; 'scrapy_redis.pipelines.RedisPipeline': 300&#125; 必须配置1 2 3 4 5 6 7 8 # 配置REDIS_URL = '(redis://redis数据库用户(默认是root)：redis数据库连接密码(默认为空))@(redis的连接地址):(redis的端口号)'# Specify the full Redis URL for connecting (optional).# If set, this takes precedence over the REDIS_HOST and REDIS_PORT settings.# hostname：redis的链接地址，由于多台电脑要连接同一个redis数据库，所以，这个链接地址可以是其中一台电脑的IP地址。注意：这个IP地址对应的电脑必须启动redis，# 如果是阿里云上购买的redis数据库服务器，这个hostname就填写阿里云的redis服务器的公网IP#REDIS_URL = 'redis://root@ip:6379'#REDIS_HOST = 'localhost'#REDIS_PORT = 6379 必须配置 配置redis服务，默认只支持localhost的本地链接，不允许远程链接，需要配置redis服务，开启远程连接功能。找到本地安装包，修改redis.windows.conf文件 将protected-mode yes 这个保护模式改为 no 将bind 127.0.0.1 修改成REDIS_URL中设置的IP地址(**) 注意：分布式爬虫程序中设置的所有的key，都是”xxx:xxx”结构，对应的是容器的名称（类似于MySQL中的表名），并不是容器内容中key：value的这个key。 > (必须配置) from ..scrapy_redis.spiders import RedisSpider class BoleSpider(RedisSpider): name = 'bole' allowed_domains = ['jobbole.com'] # start_urls = ['http://blog.jobbole.com/all-posts/'] # 分布式就不需要再设置起始的url，需要通过redis进行添加起始的url。起始的url添加到哪去了？被添加到了公共队列queue中。让多台机器中的其中一台从公共的reids队列queue中，获取起始的url，并对这个起始的url进行请求和解析，获取更多的url，然后将所有的url构造成Request，还放入公共队列queue中，让其他机器获取这些Request请求。 redis_key = 'jobbole:start_urls' > 向redis_key = 'jobbole:start_urls'这个键中，添加起始url. lpush jobbole:start_urls http://blog.jobbole.com/all-posts/ > 修改MySQL数据库为远程连接，让所有电脑连接同一个数据库，爬取出来的数据都保存在同一个数据库的表中。 默认情况下，MySQL分配的root用户只允许本地连接localhost，如果需要通过IP建立数据库的连接，需要创建一个具有远程连接权限的用户： *.*: 所有连接地址都可以使用，比如：localhost,192.168.1.121,117.56.23.4 rootuser: 新创建的用户名 '%': 表示所有权限 '123456': 连接密码 grant all privileges on *.* to rootuser@'%' identified by '123456';(需要从cmd先进入mysql，再输入这个命令) ### scrapy去重： start_url = [‘http://blog.jobbole.com/all-posts/&#39;, ‘http://blog.jobbole.com/all-posts/&#39;] 这个列表中的所有的url都不会去重。 1 2 3 4 # dont_filter的默认值是Falsedef start_request(): for start_requests: yield Request(dont_filter=True) 1 2 3 4 5 6 7 # 开始去重def enqueue_request(): # 此函数位于core/scheduler文件中。 if not dont_filter and self.request_fingerprint(request): return False else: # 将request添加到队列中 除了start_urls之外的这些请求都是要去重的。 1 # scrapy-redis去重：和scrapy默认的去重一致。]]></content>
      <categories>
        <category>scrapy</category>
      </categories>
      <tags>
        <tag>scrapy,redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[youxinSpider]]></title>
    <url>%2F2018%2F09%2F20%2Fyouxinspider-1%2F</url>
    <content type="text"><![CDATA[关于某信二手车的验证码问题。 这个页面的请求其实不多，也没遇见什么加密 所以几乎是一瞬间就发现了请求的网址 -&gt; validate，还顺便看到了它的两个参数 vcode：后面跟的是四位验证码 t：后面跟的是网页中图片验证码的链接里面的 于是乎，乐呼呼的填上了，但是直接请求失败。403状态码。还是接着研究吧。 scrapy采用跟的是多线程模式，十个线程，每一个都遇到了验证码，所以十个线程肯定都会去刷新验证码图片，要是能保证最新的验证码给服务器进行认证也行，但是没办法获取到最新的啊。##]]></content>
      <categories>
        <category>scrapy</category>
      </categories>
      <tags>
        <tag>xin.com</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于此博客图床的使用]]></title>
    <url>%2F2018%2F09%2F18%2Fgraph-bed%2F</url>
    <content type="text"><![CDATA[就像很多文章中写的那样，要是后期需要在微博中配大量的图片无疑会大大降低网页的加载速度，本来把网页托管在github pages中链接速度就很不给力，不能再被图片拖了后腿， 试过七牛云的对象存储功能，确实功能很强大、很全面。但是..要备案，用图床本来也不是刚需，加上主要原因：自己也买不起阿里的服务器。。就暂时搁置了看了其他网站基本上也都差不多的，因为国内的都需要备案嘛，国外的速度又不快。所以就自然而然的盯上了微博图床。好废话不多说，测试阶段好像也不需要，那就直接写怎么用吧，我这里用的是一款浏览器插件就叫微博图床，别人在github上的开源项目@https://github.com/Semibold/Weibo-Picture-Store 这里放上链接，有兴趣的可以去看看。使用非常简单，只需要在浏览器中登录过新浪就行了，这个插件就不需要单独配置了，直接就能上传图片，生成外链，好了废话不多说，下面的一张图就当是测试了。 还在考虑到底要不要在这里放上背景图片，因为背景图用的是@https://unsplash.com/ 这个网站的提供的接口，可以自定义相册什么的，使用非常方面，最重要的是竟然支持自定义尺寸，它会给你在服务端裁剪好。唯一的缺点就是。。在国内访问也太慢了。但是转到微博图床后，就只能使用那几个图片，而且还要手动修改尺寸，还要考虑怎么随机获得这个图片。一大堆麻烦事，开始尝试一下吧。 好了，到此结束。其实没写什么东西，就是顺便练习练习怎么写博客，以后留给自己看吧！]]></content>
      <tags>
        <tag>图床</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[新浪微博爬虫]]></title>
    <url>%2F2018%2F09%2F14%2FWeiBoSpider%2F</url>
    <content type="text"><![CDATA[爬取新浪微博时遇到的一些小问题汇总，但是由于现在还没有养成，报错就截图的好习惯，所以现在只能捡能想起来的先写点。 先说一下一开始遇到的这个错误，经过查找原因，发现可能是由的网址的拼接规则不对1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 ValueError: Missing scheme in request url: 2018-09-16 17:19:40 [scrapy.core.scraper] ERROR: Spider error processing &lt;GET https://weibo.cn/search/mblog?hideSearchFrame=&amp;keyword=%E8%B5%B5%E4%B8%BD%E9%A2%96&amp;page=2&gt; (referer: None)Traceback (most recent call last): File "F:\ENVS\scrapy_two\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback yield next(it) File "F:\ENVS\scrapy_two\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output for x in result: File "F:\ENVS\scrapy_two\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in &lt;genexpr&gt; return (_set_referer(r) for r in result or ()) File "F:\ENVS\scrapy_two\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in &lt;genexpr&gt; return (r for r in result or () if _filter(r)) File "F:\ENVS\scrapy_two\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in &lt;genexpr&gt; return (r for r in result or () if _filter(r)) File "E:\yuke\test\9-13\XinLangSpider\XinLangSpider\spiders\weibo.py", line 18, in parse yield scrapy.Request(url=url,callback=self.parse_info) File "F:\ENVS\scrapy_two\lib\site-packages\scrapy\http\request\__init__.py", line 25, in __init__ self._set_url(url) File "F:\ENVS\scrapy_two\lib\site-packages\scrapy\http\request\__init__.py", line 62, in _set_url raise ValueError('Missing scheme in request url: %s' % self._url) 发现一个可能是错误的原因的地方 修改之前 1 2 3 for every_article in page: url = every_article.xpath('div/a[@class="cc"]/@href').extract_first('') yield scrapy.Request(url=url,callback=self.parse_info) 修改之后 1 2 3 url_list = page.xpath('div/a[@class="cc"]/@href').extract()for url in url_list: yield scrapy.Request(url=url,callback=self.parse_info) 其实也就是等于以前在for循环里整理网址,修改后在for循环之外整理网址,然后遍历网址的列表.猜测可能是因为scrapy爬虫框架要求的高效率吧,这样处理会让爬虫可以同步进行爬取列表中的网址. 关于转发数、赞数和评论数的提取，下面是item里面的一部分定义函数，使数量存在时返回，不存在时返回’0’1 2 3 4 5 6 7 8 9 10 11 12 def process_zhuanfa(value): if value[3:-1] is None or value[3:-1] == '': return '0' else: return value[3:-1]def process_zan(value): return value[2:-1]def process_pinglun(value): if value[4:-2] is None or value[4:-2] == '': return '0' else: return value[4:-2] 关于cookie池的问题 采用的是别人写好的开源的一个项目，输入账号密码后，自动生成cookie，并通过@http://127.0.0.1:5000/weibo/random 来提取生成生成的值获取到的结果为str，需要反序列化成字典 request.cookies = requests.get(&apos;http://127.0.0.1:5000/weibo/random&apos;).json() 关于发布时间转成统一格式的问题 现在发现有三种格式的时间 **分钟之前(一小时之内的会这样显示) 今天 :(超过一小时就会这样显示) 09月13日 22:33 (不是今天的) 现在考虑的是,分别格式化为时间戳,然后再从时间戳转换成要求的统一格式1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 import timedef process_time(value): value = value.split(' ')[0:2] date = 'no_time' if '分钟' in value[0]: time_stamp = time.time() - int(value[0].split('分钟')[0]) * 60 date = time.strftime("%Y-%m-%d %H:%M:%S", time.localtime(time_stamp)) return date if '今天' in value[0]: local_time = time.localtime() time_string = "%s年%s月%s日 %s"%(local_time.tm_year,local_time.tm_mon,local_time.tm_mday,value[1]) date = time.strftime('%Y-%m-%d %H:%M:%S', time.strptime(time_string, '%Y年%m月%d日 %H:%M')) return date if '月' in value[0]: local_time = time.localtime() time_string = "%s年%s %s" % (local_time.tm_year, value[0], value[1]) date = time.strftime('%Y-%m-%d %H:%M:%S', time.strptime(time_string, '%Y年%m月%d日 %H:%M')) return date 最后的最后，准备接入微博的图床。也就是用人家的相册做些事情。。 下面是测试用的图片 还是不要了，现在已经改完了。。]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>scrapy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[zhihu的一些评论]]></title>
    <url>%2F2018%2F09%2F12%2Fproxy-pool%2F</url>
    <content type="text"><![CDATA[代理IP: 1.免费的代理IP,可用IP少,时效性较短,大部分的代理IP可能在访问网站都是失效的. 2.收费的代理IP,讯代理. 3.爬取国外的网站,VPN代理服务器. proxy_pool: @项目地址：https://github.com/jhao104/proxy_pool.git 1.它将国内的代理IP网站都进行了爬取; 2.代理IP爬取完毕之后,会进行检测,可用的IP会保存到数据库redis中; 3.会定期将数据库中的代理IP拿出来检测,失效的IP从数据库中删除。 4.支持扩展 练习项目:1. 解决登陆问题； 2. 使用downloadmiddleware, ua, 代理、池; 3. 使用itemloader完成数据的提取和解析； 4. 提取所有问题的信息：标题、点赞数、评论数、关注者数量、浏览数量；将列表页数据单独 保存至一个表中； 5. 提取每一个问题的所有答案：回答用户、回答内容、回答时间、回答 点赞数、回答评论数；将所有的答案保存至另外一个表中； 6. 数据库的写入要求使用异步Twisted框架完成； 首先需要先设置好代理IP和请求头,也就是自定义下载中间件然后根据需求知道要先获得登录后的Cookie 这个设置在middlewares里 cookie登录需要设置成键值对的方式 1 2 3 4 5 6 7 8 9 10 11 12 13 14 request.cookies = &#123; '_xsrf':'BKORzjqUj35ax0DdrwKnVn25RkNRaahM', '_zap':'4dc36192-e267-44d1-b8f4-074822456258', 'd_c0':"AFAlC-xzMw6PTlUrgV6b_Bu4DtkilyXo-Z4=|1536751108", 'capsion_ticket':"2|1:0|10:1536756874|14:capsion_ticket|44:MDRlODI3NDk5ZjhlNGRlOThiNzdiN2Q1NGM3OWEyNjQ=|7b1b74e68e3ccc4440577f394f8e1ded1efc6a5bca537a1a0d459b71d36877c2", 'q_c1':'cd8e72dfa9384e70af51bc5c90b10f8a|1536751149000|1536751149000', 'l_n_c':'1', 'l_cap_id':"NGRjMmM4OTZkOTBlNGQ0ZTg2MTQ2YTk4YTNiODBhMGI=|1536756854|ae8c593677b5d5d7301941fa6e133c0c4f4f1022", 'r_cap_id':"YTM4YTAyZjQ2ZDAwNDhiNjlmYTYxNmFmODU2NTMwMWU=|1536756854|b164129500acaa359cfc585c4c1d5d32b8529bf2", 'cap_id':"MjQ0ZWZhOGQwZDlhNGQ5MGE0ZWRlMTNlMWYyZDcwMWQ=|1536756854|a8f854ab2430dbb1dc0f0ee8b9cf89e9f316b556", 'n_c':'1', 'z_c0':"2|1:0|10:1536756886|4:z_c0|92:Mi4xU2JyYkF3QUFBQUFBVUNVTDdITXpEaVlBQUFCZ0FsVk5sbHFHWEFEMGZOMm5qMmItXzJlUElVWEpCWHZGWkFzMkt3|8e0843029fb42283f58a63ec8765ba96c10b936e8775f14293cf3b12501e0f13", 'tgw_l7_route':'53d8274aa4a304c1aeff9b999b2aaa0a' &#125; 登录成功之后,获取问题详情, 1.发现获取详情的网址是xhr请求,其中一条为https://www.zhihu.com/api/v3/feed/topstory?action_feed=True&amp;limit=7&amp;session_token=b36c08db4577ef8d563788f5337b283c&amp;action=down&amp;after_id=13&amp;desktop=true 2.经解析参数发现session_tokon这个参数,每次启动项目都会不一样,所以尝试在主页面的response中寻找.最终解析该值得代码为: 1 2 3 4 data = response.xpath('//div[@id="data"]/@data-state').extract_first('')pattern = re.compile(r'sessionToken":"(.*?)"',re.S)session_token = re.findall(pattern,data)print(session_token) 3.参数after_id的值为每一次加7,所以设置range循环,步进为7 yield回调解析json数据的函数.经测试发现有的问题,没有question的值.–这个问题是因为: 有的是文章,有的是问题.–这里暂时只解析那些是问题的页面. 网址都是跟据id拼接出来的 然后发现https://www.zhihu.com/question/41435689/answer/465837326拼接出这样的网址是无法获取到全部的回答的.所以..还是考虑把后面的回答者去掉 变成https://www.zhihu.com/question/41435689这样 解析问题页面的评论 1.https://www.zhihu.com/api/v4/questions/41435689/answers?发现xhr请求中类似这样的get请求是获取的评论. 2.进去后经简单解析,发现没个请求有四个,所以就用从0开始步进为5的循环.–以后可以考虑获取一下评论的总数,然后按总数获取. 这一点参数里有两个值offset=5&amp;limit=5这两个控制每次获取的评论数.暂时不需要更改 3.include=data%5B*%5D这个参数,好像没个问题的也都不一样. 关于这一点,参数里面需要这些东西的都是用[]代替的,写出[]也能正常获取评论 4.scrapy获取的json数据我用的解析方式,这样就能获取到字典了 1 2 response_data = response.body.decode('utf-8')json_data = json.loads(str(response_data)) 5.关于评论帖子的时间这一块,返回的好像是个时间戳,需要反序列化一下 1 2 3 4 5 6 7 8 9 import timeif 'updated_time' in data: # 有更新时间就获取更新时间 date = data.get('updated_time','')else: # 没有就获取创建时间 date = data.get('created_time','')timeArray = time.localtime(date)date = time.strftime("%Y-%m-%d %H:%M:%S", timeArray) 关于遇到的一些错误1 MySQLdb TypeError: %d format: a number is required, not str问题解决 这个其实就是传参数的时候占位符填错了,一般都是因为传int类型时不需要用%d,直接用%s就搞定了,亲测成功~! 还有一点就是最好要把str类型的占位符改成’%s’,就是带引号的这种,防止1146错误..]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>scrapy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[用twisted异步写入]]></title>
    <url>%2F2018%2F09%2F11%2Fuse_twisted%2F</url>
    <content type="text"><![CDATA[异步写入MySQL数据库,如果scrapy解析数据的速度远远超过数据库的写入速度,那么很容易就造成数据的丢失.这里用twisted保证数据的异步多线程写入,提高写入速度.详情请看正文@本文相关源码: https://github.com/Loveyueliang/yuke/tree/master/9-11/jobbolespider 首先需要先导入模块1 from twisted.enterprise import adbapi 接着在自定义的pipeline里面重构from_settings函数1 2 3 4 5 @classmethoddef from_settings(cls, settings): """ 这个函数名称是固定的，当爬虫启动的时候，scrapy会自动调用这些函数，加载配置数据。 """ 创建一个数据库连接池对象，这个连接池中可以包含多个connect链接对象。1 2 3 # 参数1：操作数据库的包名# 参数2：链接数据库的参数db_connect_pool = adbapi.ConnectionPool('pymysql', **params) 初始化这个类的对象1 obj = cls(db_connect_pool) 在process_item这个函数里执行数据的多线程写入操作1 2 3 4 5 # 参数1：在线程中被执行的sql语句# 参数2：要保存的数据result = self.dbpool.runInteraction(self.insert, item)# 给result绑定一个回调函数，用于监听错误信息result.addErrback(self.error) 定义一个打印错误信息的函数 1 2 def error(self, reason): print('--------', reason) 定义一个执行插入语句的函数,用twisted的好处其中一点就是不用一步步的手动提交 1 2 3 4 def insert(self, cursor, item): insert_sql = "INSERT INTO bole(bole_title, bole_date, bole_tag, bole_content, bole_dz, bole_sc, bole_pl, bole_img_src) VALUES (%s, %s, %s, %s, %s, %s, %s, %s)" cursor.execute(insert_sql, (item['bole_title'], item['bole_date'], item['bole_tag'], item['bole_content'], item['bole_dz'], item['bole_sc'], item['bole_pl'], item['bole_img_path'])) # 不需要commit()]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>scrapy,mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于51job的爬虫说明]]></title>
    <url>%2F2018%2F09%2F10%2Ftest_pic%2F</url>
    <content type="text"><![CDATA[@本文源码：https://github.com/Loveyueliang/yuke/tree/master/9-10 错误点: 感觉唯一的坑就是刚学的sql插入,一开始定义的表的字段名和item定义的不一样,所以就各种报KeyError.1 2 insert_sql = &quot;INSERT INTO job(zwmc, zwxz, gzdd, gzyq)VALUE (&apos;%s&apos;, &apos;%s&apos;, &apos;%s&apos;, &apos;%s&apos;)&quot;%(item[&apos;zwmc&apos;],item[&apos;zwxz&apos;],item[&apos;gzdd&apos;],item[&apos;gzyq&apos;]) self.cursor.execute(insert_sql) 最终结果还不错,暂时就爬取这一点信息吧.]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>scrapy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[xieyuying]]></title>
    <url>%2F2018%2F09%2F07%2Fxieyuying%2F</url>
    <content type="text"><![CDATA[@本文源码：https://github.com/Loveyueliang/XieYuYing.git si: 这里面写一写个人需求或建议的一写东西.调用剧情,选择用1-11这种类似的东西关于人物类的创建,人物可以考虑做在一个文件里,共同继承一个父类People –现在这一项已经完成.关于玩家名字的说明: 调用更改玩家名字的时候, 直接用赋值语句,现有判定输入只能为中文名字关于好感度的说明:好感度设计的逻辑一开始没有理清.现已完成 1.0版本–定义了一个好感度的类,方法为增加/减少好感度,目前是女主和胖老板的父类.理想参数为执行的相应动作,返回值为增加/减少的好感度. 但是目前还没有定义需要的所需要的以动作和数字为键值对的字典.动作多的话倒是可以采用这种方法, 要是很少的几个动作,估计要重新给每一个动作编写一个方法 好感度采用等差数列递增的方式.多引用一个增长速度 2.0最新进度,可以采用函数右移的方法来实现偶尔的坏话处理,,把函数做成列表生成器,达到每调用一次就可以得到下一个结果 生成器也做好了,但是需要一个初始的好感度,也就是说,,先给每个人用生成器生成到相应想给到的好感度 3.0最终版本:终于搞定了 好感度最终定义为一个列表增加好感度的方法里面先查询现在好感度的索引,然后通过索引加1的方法调用下一个应该得到的好感度. 关于商品的说明: 现在还没有开始做不过估计要创建一个商品的类,方法的话,应该就只有购买,不过属性可能会有点多. 需要注意: 要添加的一个功能–通过购买可以选择删除剧情中的某个人物,感觉关联度有点高]]></content>
      <categories>
        <category>类的使用</category>
      </categories>
      <tags>
        <tag>面向对象</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机开机]]></title>
    <url>%2F2018%2F09%2F06%2Fcapture_start%2F</url>
    <content type="text"><![CDATA[计算机启动过程 BIOS简介: 基本的输入输出系统,连接底层的硬件和系统 第一步: 按下电源键 按下电源键的时候电源开始向主板和cpu供电,一开始电压不稳定,主板控制芯片会一直向cpu发送reset信号,让cpu初始化 一般这个过程非常短暂,几乎在一瞬间电压就会稳定,控制芯片撤掉reset信号.但按下reset的时候,这个过程就特别明显…. 第二步: BIOS硬件自检–检测关键性设备 BIOS开始上电自检,这个地方会主要会检测内存,显卡,要是检测不通过,蜂鸣器会报告检测到的错误,一般用滴的长短和次数来表示错误的类型 这个地方检测的内存地址只有640k. 第三步: 屏幕亮,检测显卡BOIS 这里系统BIOS会读取显卡的BIOS启动显卡,此时屏幕开始亮起,会简单的显示显卡的一些参数, 要是不亮,就… 第四步: 显示BIOS自己的界面 会显示BIOS日期,主板芯片组型号,主板识别码,主板厂商,等一堆东西. 第五步: 仔细检测cpu和内存 会把检测到的cpu类型,主频打印在屏幕上 下面会跟着开始检测所有内存信息 第六步: 检测标准硬件设备 检测硬盘 cd-rom 软驱 并行接口,和串行接口 第七步: 检测外围设备 检测那些可以即插即用的东西,比如usb接口的U盘什么的.检测到后为每一个检测到的设备分配dma.i/o接口等 每检测到一个设备,都会把信息打印在屏幕上 第八步: BIOS清屏,重新显示系统配置表 会在系统上方显示一个概括性的配置列表,列出了系统安装的各种各种标准硬件设备以及他们使用的资源和相关参数. 第九步: 检测硬件更新 这里的硬件指的是类似外接内存,声卡,网卡,显卡,等设备有没有被更换,如果更换了,就更新一下存储,方面系统读取配置. 第十步: 根据启动顺序,启动引导 这一步,才开始引导操作系统,系统BIOS读取并执行硬盘的主引导记录,从分区表中找到第一个活动分区,然后读取并执行分区的引导记录.分区的引导记录负责读取并执行系统 下面是结构图]]></content>
      <categories>
        <category>前端</category>
      </categories>
      <tags>
        <tag>capture</tag>
      </tags>
  </entry>
</search>
