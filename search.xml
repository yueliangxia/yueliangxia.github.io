<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[天眼查的字体加密]]></title>
    <url>%2F2018%2F10%2F09%2Ftianyancha%2F</url>
    <content type="text"><![CDATA[关于一些网站的字体加密方面，目前发现的也就只有两种：一种是起点网的那个总字数那种的给的是是禁止的数据，转换成十六进制之后再在woff文件转换成的xml文件中寻找对应的关系。另一种就是类似于天眼查的，获取到的源码是字体的位置信息，然后需要根据位置信息取出对应的数据。 前提： 这里我用了两种方式来搞定这种相对应位置的字体加密。 1.第一种的思路是：主要采用PIL图像处理模块来根据1-0这个十个源码里获得的数字取出字体文件里真正要显示的数字，并画在画布上保存成文件，然后利用图片文字识别模块(pytesseract)来取出保存的图像中包含的数字，从而获得它们的对应关系。 2.第二种思路就是还是解析字体转换成的xml文件，从中找到它们之间的对应关系。]]></content>
  </entry>
  <entry>
    <title><![CDATA[Django的学习记录]]></title>
    <url>%2F2018%2F09%2F28%2Fdjango-study%2F</url>
    <content type="text"><![CDATA[Django的一些基础知识，例如Django框架的基本结构 (1)为什么要选择Django1.一站式的解决方案（ORM，Session，Admin等） 可以做后台的定时任务，执行相关的任务；也可以做运维的管理后台，可以快速的搭建起一个后台的模型 Django的ORM是值得肯定的。因为有了它的ORM模型，我们可以方便的调用modles，直接去操作数据库，这样省去了手写SQL，或者SQL安全方面的一些问题 2.成熟的Python Web框架（Django社区、丰富模块、稳定） 相对于现在主流的其他几款Python Web框架而言，Django开源于05年，所以相对于现在来说比较早，就因为这一点，他有着丰富的Django社区。对于出现的问题可以快速的查找到、定位到相应的问题，找到解决方案，对于新手而言，也可以快速的上手，进行学习。 对于新手主要要去浏览Django的官方文档 官方文档：https://www.djangoproject.com/，里面有Django的整套不同版本的文档，主要是英文的。但是文档里从基础一直讲到相当细节的内容。对于开发者而言是相当方便的。当然了要想看中文的可以去百度搜索Django中文文档。 模块：有了丰富的模块，可以开发出一些多样的内容，省去了需要自己从零开发代码和相关的一些底层的设计所需要的耗时。 (2)Django工程结构和建立 首先安装好Django后，通过Django-admin startproject 项目名 新建一个项目。 应用的建立(scanhosts：应用名)Ubuntu: ./manage.py startapp scanhostsWindows: python manage.py startapp scanhosts 下面是Django的目录的大体结构： 首先最外层的是容器项目名–注意：这个容器项目，没有什么实际的作用，修改容器项目的名字，并不会影响后面项目的运行。 manage.py：是一个命令行工具，会接收对于自己的Django工程的管理，比如Django工程的启动和关闭，或者进行相应的调试，或者是数据模型的迁移等等操作，都是用这个文件进行操作。–里面的内容很简单就是从终端接收对应的参数去执行对应的命令，所以起到的是一个Django的管理工具的作用。 setting.py：详细配置非常重要！这个里面有Django工程里面初始化的所有的相应的默认设置，也有后面我们要对Django工程对应的核心模块的一些设置都会集中在settings这个模块里面。Django初始化启动的服务也都是通过settings里面对应的配置去读取urls.py: 用于做Django工程的url路由的——我们要做自己的Web后台服务的时候，要生成自己的http的url，urls.py就是url的路径进行对应的路由的。wsgi.py：这个是Django自己的一个与wsgi兼容Web服务的接口。不用做详细的了解其他文件夹：然后下面就是包含的应用项目，一个容器项目里面可以包含多个应用项目scanhosts：自己建立的应用models.py：重点了解 Django的ORM模型建立model这个模型的一些配置都会放到这里面,默认里面只有下面这一行代码。这里是指导入了Django的模型，我们在定义数据库模型的时候，就要在这个文件里建立。1 2 from django.db import models# Create your models here. migrations文件夹：默认是空的，这里是要做数据迁移用的。我们在models这个文件里进行对应的数据库模型的建立和更改以后，为了进行一个对应的迁移操作，会生成一些临时文件（一些临时的但是非常重要的中间文件）在migrations里面。会保存对应的文件的操作信息在这个文件夹里面。views.py： 是一个视图文件，同样也是在我们如果需要进行Web后台或者接口服务的话，对应相应的逻辑处理的逻辑层的代码书写都会放在这个文件里。 (3)第一个DevOPS工程 启动Django工程的命令python manage.py sunserver settings文件详细配置 1 2 3 # Build paths inside the project like this: os.path.join(BASE_DIR, ...)# BASE_DIR 是一个路径，工程启动的初始化基础路径，后面会调用基础路径去取其他的文件BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__))) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # 新建的应用如果没有加到settings文件里是不会随着Django工程一起启动的。所以首先要做的就是把自己的项目添加到INSTALLED_APPS这个列表里，最好直接添加到最后面。# Application definitionINSTALLED_APPS = [ # 前面都是Django的一起默认的基础模块 'django.contrib.admin', # 后台模块 'django.contrib.auth', # 认证相关的模块 'django.contrib.contenttypes', 'django.contrib.sessions', 'django.contrib.messages', 'django.contrib.staticfiles', 'scanhosts',] 1 2 3 4 5 6 7 8 9 10 # 数据库的连接，也是需要配置的。默认使用的是sqlite的数据库# Database# https://docs.djangoproject.com/en/2.1/ref/settings/#databasesDATABASES = &#123; 'default': &#123; 'ENGINE': 'django.db.backends.sqlite3', 'NAME': os.path.join(BASE_DIR, 'db.sqlite3'), &#125;&#125; (4)Django日志logging模块(5)Django邮件发送 极验验证： 抓取极验参数: 任何一个网站，如果在登录时网站接入的极验的接口，那么该网站就可以使用极验验证码进行登录。此时，极验验证码API就会返回两个极验参数(gt和challenge)。这两个参数只跟极验验证码API相关，跟这个网站没有任何关系。 注意： 有的网站是直接调用极验官方提供的验证码接口，比如：极验的官方后台：https://auth.geetest.cn/api/ ；有的网站又对极验验证吗接口封装了一个API接，比如：魅族登录：https://login.flyme.cn/sec/geetest? 将获取的极验参数，提交给极验破解网站的识别接口。会得到新的返回值： 1 2 3 4 5 &#123; status: "ok", challenge: "3d033f099597f5ae63e2e2c902301d183z", validate: "8f6ebd56291ed6569ac40c1d74780985"&#125; 将上述参数challenge换个validate，混合着网站自己的提交的参数向网站自己的url发生POST请求，即可。]]></content>
      <categories>
        <category>Django</category>
      </categories>
      <tags>
        <tag>Django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[scrapy部署爬虫项目]]></title>
    <url>%2F2018%2F09%2F27%2Fscrapy-overview%2F</url>
    <content type="text"><![CDATA[功能：它就相当于是一个服务器，用于将自己本地的爬虫代码，打包上传到服务器上，让这个爬虫在服务器上运行，可以实现对爬虫的远程管理。(远程启动爬虫，远程关闭爬虫，远程查看爬虫的一些日志。) 1.scrapy的安装pip install scrapy 2.如何将本地的爬虫项目Deloying(打包)，上传至scrapyd这个服务中。 a&gt; 提供了一个客户端工具，就是scrapyd-client，使用这个工具对scrapyd这个服务进行操作，比如向scrapy这个服务打包上传项目。scrapy-client类似于redis-cli.exe,mongodb数据库的client。pip install scrapyd-client==1.2.0a1 3.上述服务和客服端安装好之后，就可以启动scrapyd这个服务了。服务启动之后，不要关闭，访问 http://127.0.0.1:6800/ 出现下面的页面正常 4.配置爬虫项目，完成以后，再通过addversion.json进行打包。 5.上述的scrapyd服务窗口cmd不要关闭，再心打开一个cmd窗口，用于使用scrapyd-client客户端连接scrapyd服务。 进入项目根目录，然后输入scrapyd-delopy命令，查看scrapyd-client客户端命令是否正常可用 1 2 输入：scrapyd-deploy返回：Unknown target: default 查看当前可用于打包上传的爬虫项目： 1 2 输入：scrapyd-deploy -l返回：bole(scrapy.cfg里添加的爬虫名称) http://localhost:6800/(scrapy.cfg里解注释的url) 使用scrapy-deploy命令打包项目：scrapyd-deploy bole -p jobbolespider 参数：Status:”ok”/“error” 项目上传状态Project: 上传的项目名称Version: 项目的版本号，值是时间戳Spiders: 项目project包含的爬虫个数 通过API接口，查看已经上传至scrapyd服务项目。1 命令：curl http://localhost:6800/listprojects.json` 键值： Projects:[]所有已经上传的爬虫项目，都会显示在这个列表中。 通过API接口，查看某一个项目中的所有的爬虫名称； 1 命令： $ curl http://localhost:6800/listspiders.json?project=myproject 通过API接口，启动爬虫项目： 1 命令： curl http://localhost:6800/schedule.json -d project=爬虫项目名称名称 -d spider=项目中某一个爬虫名称 键值： jobid：是根据项目(jobbolespider)和爬虫(bole)生成的一个id，将来用于取消爬虫任务。1 取消：curl http://localhost6800/cancel.json -d project=jobbolespider -d job=** 1 删除命令：$ curl http://localhost:6800/delproject.json -d project=myproject 注意：如果项目上传失败，需要先将爬虫项目中打包生成的文件删除(build、project.egg-info、setup.py) 注意：删除前一定要先取消！]]></content>
      <categories>
        <category>scrapy</category>
      </categories>
      <tags>
        <tag>scrapy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式爬虫]]></title>
    <url>%2F2018%2F09%2F25%2Fscrapy-redis%2F</url>
    <content type="text"><![CDATA[1. 概念将同一个爬虫程序放在多台电脑上(或者同一个电脑中的多个虚拟机环境)，并且在多台电脑上同时启动这个爬虫。异态电脑运行一个爬虫程序成为单机爬虫。 2. 作用可以利用多台电脑的带宽，处理器等资源提高爬虫的爬取速度； 3. 原理分布式需要解决的问题（共用的队列Queue和共用的去重集合set(),这两个是scrapy自身无法解决的问题。只能通过第三方组件。） 需要保证多台电脑中的爬虫代码完全一致； 多台电脑操作统一网站，如何管理url去重？ scrapy如何做去重的？scrapy会将所有的Request对象(request.url,request.method,request.body)进行加密，生成一个指纹对象fingerprint；然后将指纹对象放入set()集合中进行对比，如果set()集合中已经存在这个对象，那么scrapy就会将Request抛弃。如果set()集合中不存在这个指纹对象，就将这个Request添加到调度队列queue中，等待被调度器调度； 多台电脑就会有多个set()对象。能否使用各自的set()做去重？ 不能。这个set()对象中的数据都是保存在电脑内存中的，电脑的内存空间是不能共享的。 这个set()对象随着程序的启动而创建，程序的退出而销毁所以，要解决这个问题需要让所有电脑公用同一个set()集合，不再使用scrapy内置的set()对象，而是使用scrapy-redis蒋政set()集合在redis中创建。然后多台电脑访问同一个redis就可以实现set集合的共用。 用于存放合法Request请求对象的对象，在scrapy中也是默认存放在内存中的，也是无法实现多台电脑的共享队列。如果是多台电脑的话，就需要保证多台电脑有共用的队列queue，这样可以保证所有电脑都是从同一队列中获取Request对象进行调度，多有set()过滤出来的Request都放到同一队列中。 所以，要解决这个问题，需要让所有电脑公用同一个队列，不再使用scrapy内置的queue，而是使用scrapy-redis将这个queue在redis中创建，然后多台电脑访问同一个redis就可以使用queue的共用。如何将不同电脑上获取的数据，保存在同一个数据库中。 4. 配置 必须配置1 2 3 4 # Enables scheduling storing requests queue in redis.SCHEDULER = "scrapy_redis.scheduler.Scheduler"# Ensure all spiders share same duplicates filter through redis.DUPEFILTER_CLASS = "scrapy_redis.dupefilter.RFPDupeFilter" 可选配置1 2 3 4 # Store scraped item in redis for post-processing.ITEM_PIPELINES = &#123; 'scrapy_redis.pipelines.RedisPipeline': 300&#125; 必须配置1 2 3 4 5 6 7 8 # 配置REDIS_URL = '(redis://redis数据库用户(默认是root)：redis数据库连接密码(默认为空))@(redis的连接地址):(redis的端口号)'# Specify the full Redis URL for connecting (optional).# If set, this takes precedence over the REDIS_HOST and REDIS_PORT settings.# hostname：redis的链接地址，由于多台电脑要连接同一个redis数据库，所以，这个链接地址可以是其中一台电脑的IP地址。注意：这个IP地址对应的电脑必须启动redis，# 如果是阿里云上购买的redis数据库服务器，这个hostname就填写阿里云的redis服务器的公网IP#REDIS_URL = 'redis://root@ip:6379'#REDIS_HOST = 'localhost'#REDIS_PORT = 6379 必须配置 配置redis服务，默认只支持localhost的本地链接，不允许远程链接，需要配置redis服务，开启远程连接功能。找到本地安装包，修改redis.windows.conf文件 将protected-mode yes 这个保护模式改为 no 将bind 127.0.0.1 修改成REDIS_URL中设置的IP地址(**) 注意：分布式爬虫程序中设置的所有的key，都是”xxx:xxx”结构，对应的是容器的名称（类似于MySQL中的表名），并不是容器内容中key：value的这个key。 > (必须配置) from ..scrapy_redis.spiders import RedisSpider class BoleSpider(RedisSpider): name = 'bole' allowed_domains = ['jobbole.com'] # start_urls = ['http://blog.jobbole.com/all-posts/'] # 分布式就不需要再设置起始的url，需要通过redis进行添加起始的url。起始的url添加到哪去了？被添加到了公共队列queue中。让多台机器中的其中一台从公共的reids队列queue中，获取起始的url，并对这个起始的url进行请求和解析，获取更多的url，然后将所有的url构造成Request，还放入公共队列queue中，让其他机器获取这些Request请求。 redis_key = 'jobbole:start_urls' > 向redis_key = 'jobbole:start_urls'这个键中，添加起始url. lpush jobbole:start_urls http://blog.jobbole.com/all-posts/ > 修改MySQL数据库为远程连接，让所有电脑连接同一个数据库，爬取出来的数据都保存在同一个数据库的表中。 默认情况下，MySQL分配的root用户只允许本地连接localhost，如果需要通过IP建立数据库的连接，需要创建一个具有远程连接权限的用户： *.*: 所有连接地址都可以使用，比如：localhost,192.168.1.121,117.56.23.4 rootuser: 新创建的用户名 '%': 表示所有权限 '123456': 连接密码 grant all privileges on *.* to rootuser@'%' identified by '123456';(需要从cmd先进入mysql，再输入这个命令) ### scrapy去重： start_url = [‘http://blog.jobbole.com/all-posts/&#39;, ‘http://blog.jobbole.com/all-posts/&#39;] 这个列表中的所有的url都不会去重。 1 2 3 4 # dont_filter的默认值是Falsedef start_request(): for start_requests: yield Request(dont_filter=True) 1 2 3 4 5 6 7 # 开始去重def enqueue_request(): # 此函数位于core/scheduler文件中。 if not dont_filter and self.request_fingerprint(request): return False else: # 将request添加到队列中 除了start_urls之外的这些请求都是要去重的。 1 # scrapy-redis去重：和scrapy默认的去重一致。]]></content>
      <categories>
        <category>scrapy</category>
      </categories>
      <tags>
        <tag>scrapy,redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[youxinSpider]]></title>
    <url>%2F2018%2F09%2F20%2Fyouxinspider-1%2F</url>
    <content type="text"><![CDATA[关于某信二手车的验证码问题。 这个页面的请求其实不多，也没遇见什么加密 所以几乎是一瞬间就发现了请求的网址 -&gt; validate，还顺便看到了它的两个参数 vcode：后面跟的是四位验证码 t：后面跟的是网页中图片验证码的链接里面的 于是乎，乐呼呼的填上了，但是直接请求失败。403状态码。还是接着研究吧。 scrapy采用跟的是多线程模式，十个线程，每一个都遇到了验证码，所以十个线程肯定都会去刷新验证码图片，要是能保证最新的验证码给服务器进行认证也行，但是没办法获取到最新的啊。##]]></content>
      <categories>
        <category>scrapy</category>
      </categories>
      <tags>
        <tag>xin.com</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于此博客图床的使用]]></title>
    <url>%2F2018%2F09%2F18%2Fgraph-bed%2F</url>
    <content type="text"><![CDATA[就像很多文章中写的那样，要是后期需要在微博中配大量的图片无疑会大大降低网页的加载速度，本来把网页托管在github pages中链接速度就很不给力，不能再被图片拖了后腿， 试过七牛云的对象存储功能，确实功能很强大、很全面。但是..要备案，用图床本来也不是刚需，加上主要原因：自己也买不起阿里的服务器。。就暂时搁置了看了其他网站基本上也都差不多的，因为国内的都需要备案嘛，国外的速度又不快。所以就自然而然的盯上了微博图床。好废话不多说，测试阶段好像也不需要，那就直接写怎么用吧，我这里用的是一款浏览器插件就叫微博图床，别人在github上的开源项目@https://github.com/Semibold/Weibo-Picture-Store 这里放上链接，有兴趣的可以去看看。使用非常简单，只需要在浏览器中登录过新浪就行了，这个插件就不需要单独配置了，直接就能上传图片，生成外链，好了废话不多说，下面的一张图就当是测试了。 还在考虑到底要不要在这里放上背景图片，因为背景图用的是@https://unsplash.com/ 这个网站的提供的接口，可以自定义相册什么的，使用非常方面，最重要的是竟然支持自定义尺寸，它会给你在服务端裁剪好。唯一的缺点就是。。在国内访问也太慢了。但是转到微博图床后，就只能使用那几个图片，而且还要手动修改尺寸，还要考虑怎么随机获得这个图片。一大堆麻烦事，开始尝试一下吧。 好了，到此结束。其实没写什么东西，就是顺便练习练习怎么写博客，以后留给自己看吧！]]></content>
      <tags>
        <tag>图床</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[新浪微博爬虫]]></title>
    <url>%2F2018%2F09%2F14%2FWeiBoSpider%2F</url>
    <content type="text"><![CDATA[爬取新浪微博时遇到的一些小问题汇总，但是由于现在还没有养成，报错就截图的好习惯，所以现在只能捡能想起来的先写点。 先说一下一开始遇到的这个错误，经过查找原因，发现可能是由的网址的拼接规则不对1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 ValueError: Missing scheme in request url: 2018-09-16 17:19:40 [scrapy.core.scraper] ERROR: Spider error processing &lt;GET https://weibo.cn/search/mblog?hideSearchFrame=&amp;keyword=%E8%B5%B5%E4%B8%BD%E9%A2%96&amp;page=2&gt; (referer: None)Traceback (most recent call last): File "F:\ENVS\scrapy_two\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback yield next(it) File "F:\ENVS\scrapy_two\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output for x in result: File "F:\ENVS\scrapy_two\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in &lt;genexpr&gt; return (_set_referer(r) for r in result or ()) File "F:\ENVS\scrapy_two\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in &lt;genexpr&gt; return (r for r in result or () if _filter(r)) File "F:\ENVS\scrapy_two\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in &lt;genexpr&gt; return (r for r in result or () if _filter(r)) File "E:\yuke\test\9-13\XinLangSpider\XinLangSpider\spiders\weibo.py", line 18, in parse yield scrapy.Request(url=url,callback=self.parse_info) File "F:\ENVS\scrapy_two\lib\site-packages\scrapy\http\request\__init__.py", line 25, in __init__ self._set_url(url) File "F:\ENVS\scrapy_two\lib\site-packages\scrapy\http\request\__init__.py", line 62, in _set_url raise ValueError('Missing scheme in request url: %s' % self._url) 发现一个可能是错误的原因的地方 修改之前 1 2 3 for every_article in page: url = every_article.xpath('div/a[@class="cc"]/@href').extract_first('') yield scrapy.Request(url=url,callback=self.parse_info) 修改之后 1 2 3 url_list = page.xpath('div/a[@class="cc"]/@href').extract()for url in url_list: yield scrapy.Request(url=url,callback=self.parse_info) 其实也就是等于以前在for循环里整理网址,修改后在for循环之外整理网址,然后遍历网址的列表.猜测可能是因为scrapy爬虫框架要求的高效率吧,这样处理会让爬虫可以同步进行爬取列表中的网址. 关于转发数、赞数和评论数的提取，下面是item里面的一部分定义函数，使数量存在时返回，不存在时返回’0’1 2 3 4 5 6 7 8 9 10 11 12 def process_zhuanfa(value): if value[3:-1] is None or value[3:-1] == '': return '0' else: return value[3:-1]def process_zan(value): return value[2:-1]def process_pinglun(value): if value[4:-2] is None or value[4:-2] == '': return '0' else: return value[4:-2] 关于cookie池的问题 采用的是别人写好的开源的一个项目，输入账号密码后，自动生成cookie，并通过@http://127.0.0.1:5000/weibo/random 来提取生成生成的值获取到的结果为str，需要反序列化成字典 request.cookies = requests.get(&apos;http://127.0.0.1:5000/weibo/random&apos;).json() 关于发布时间转成统一格式的问题 现在发现有三种格式的时间 **分钟之前(一小时之内的会这样显示) 今天 :(超过一小时就会这样显示) 09月13日 22:33 (不是今天的) 现在考虑的是,分别格式化为时间戳,然后再从时间戳转换成要求的统一格式1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 import timedef process_time(value): value = value.split(' ')[0:2] date = 'no_time' if '分钟' in value[0]: time_stamp = time.time() - int(value[0].split('分钟')[0]) * 60 date = time.strftime("%Y-%m-%d %H:%M:%S", time.localtime(time_stamp)) return date if '今天' in value[0]: local_time = time.localtime() time_string = "%s年%s月%s日 %s"%(local_time.tm_year,local_time.tm_mon,local_time.tm_mday,value[1]) date = time.strftime('%Y-%m-%d %H:%M:%S', time.strptime(time_string, '%Y年%m月%d日 %H:%M')) return date if '月' in value[0]: local_time = time.localtime() time_string = "%s年%s %s" % (local_time.tm_year, value[0], value[1]) date = time.strftime('%Y-%m-%d %H:%M:%S', time.strptime(time_string, '%Y年%m月%d日 %H:%M')) return date 最后的最后，准备接入微博的图床。也就是用人家的相册做些事情。。 下面是测试用的图片 还是不要了，现在已经改完了。。]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>scrapy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[zhihu的一些评论]]></title>
    <url>%2F2018%2F09%2F12%2Fproxy-pool%2F</url>
    <content type="text"><![CDATA[代理IP: 1.免费的代理IP,可用IP少,时效性较短,大部分的代理IP可能在访问网站都是失效的. 2.收费的代理IP,讯代理. 3.爬取国外的网站,VPN代理服务器. proxy_pool: @项目地址：https://github.com/jhao104/proxy_pool.git 1.它将国内的代理IP网站都进行了爬取; 2.代理IP爬取完毕之后,会进行检测,可用的IP会保存到数据库redis中; 3.会定期将数据库中的代理IP拿出来检测,失效的IP从数据库中删除。 4.支持扩展 练习项目:1. 解决登陆问题； 2. 使用downloadmiddleware, ua, 代理、池; 3. 使用itemloader完成数据的提取和解析； 4. 提取所有问题的信息：标题、点赞数、评论数、关注者数量、浏览数量；将列表页数据单独 保存至一个表中； 5. 提取每一个问题的所有答案：回答用户、回答内容、回答时间、回答 点赞数、回答评论数；将所有的答案保存至另外一个表中； 6. 数据库的写入要求使用异步Twisted框架完成； 首先需要先设置好代理IP和请求头,也就是自定义下载中间件然后根据需求知道要先获得登录后的Cookie 这个设置在middlewares里 cookie登录需要设置成键值对的方式 1 2 3 4 5 6 7 8 9 10 11 12 13 14 request.cookies = &#123; '_xsrf':'BKORzjqUj35ax0DdrwKnVn25RkNRaahM', '_zap':'4dc36192-e267-44d1-b8f4-074822456258', 'd_c0':"AFAlC-xzMw6PTlUrgV6b_Bu4DtkilyXo-Z4=|1536751108", 'capsion_ticket':"2|1:0|10:1536756874|14:capsion_ticket|44:MDRlODI3NDk5ZjhlNGRlOThiNzdiN2Q1NGM3OWEyNjQ=|7b1b74e68e3ccc4440577f394f8e1ded1efc6a5bca537a1a0d459b71d36877c2", 'q_c1':'cd8e72dfa9384e70af51bc5c90b10f8a|1536751149000|1536751149000', 'l_n_c':'1', 'l_cap_id':"NGRjMmM4OTZkOTBlNGQ0ZTg2MTQ2YTk4YTNiODBhMGI=|1536756854|ae8c593677b5d5d7301941fa6e133c0c4f4f1022", 'r_cap_id':"YTM4YTAyZjQ2ZDAwNDhiNjlmYTYxNmFmODU2NTMwMWU=|1536756854|b164129500acaa359cfc585c4c1d5d32b8529bf2", 'cap_id':"MjQ0ZWZhOGQwZDlhNGQ5MGE0ZWRlMTNlMWYyZDcwMWQ=|1536756854|a8f854ab2430dbb1dc0f0ee8b9cf89e9f316b556", 'n_c':'1', 'z_c0':"2|1:0|10:1536756886|4:z_c0|92:Mi4xU2JyYkF3QUFBQUFBVUNVTDdITXpEaVlBQUFCZ0FsVk5sbHFHWEFEMGZOMm5qMmItXzJlUElVWEpCWHZGWkFzMkt3|8e0843029fb42283f58a63ec8765ba96c10b936e8775f14293cf3b12501e0f13", 'tgw_l7_route':'53d8274aa4a304c1aeff9b999b2aaa0a' &#125; 登录成功之后,获取问题详情, 1.发现获取详情的网址是xhr请求,其中一条为https://www.zhihu.com/api/v3/feed/topstory?action_feed=True&amp;limit=7&amp;session_token=b36c08db4577ef8d563788f5337b283c&amp;action=down&amp;after_id=13&amp;desktop=true 2.经解析参数发现session_tokon这个参数,每次启动项目都会不一样,所以尝试在主页面的response中寻找.最终解析该值得代码为: 1 2 3 4 data = response.xpath('//div[@id="data"]/@data-state').extract_first('')pattern = re.compile(r'sessionToken":"(.*?)"',re.S)session_token = re.findall(pattern,data)print(session_token) 3.参数after_id的值为每一次加7,所以设置range循环,步进为7 yield回调解析json数据的函数.经测试发现有的问题,没有question的值.–这个问题是因为: 有的是文章,有的是问题.–这里暂时只解析那些是问题的页面. 网址都是跟据id拼接出来的 然后发现https://www.zhihu.com/question/41435689/answer/465837326拼接出这样的网址是无法获取到全部的回答的.所以..还是考虑把后面的回答者去掉 变成https://www.zhihu.com/question/41435689这样 解析问题页面的评论 1.https://www.zhihu.com/api/v4/questions/41435689/answers?发现xhr请求中类似这样的get请求是获取的评论. 2.进去后经简单解析,发现没个请求有四个,所以就用从0开始步进为5的循环.–以后可以考虑获取一下评论的总数,然后按总数获取. 这一点参数里有两个值offset=5&amp;limit=5这两个控制每次获取的评论数.暂时不需要更改 3.include=data%5B*%5D这个参数,好像没个问题的也都不一样. 关于这一点,参数里面需要这些东西的都是用[]代替的,写出[]也能正常获取评论 4.scrapy获取的json数据我用的解析方式,这样就能获取到字典了 1 2 response_data = response.body.decode('utf-8')json_data = json.loads(str(response_data)) 5.关于评论帖子的时间这一块,返回的好像是个时间戳,需要反序列化一下 1 2 3 4 5 6 7 8 9 import timeif 'updated_time' in data: # 有更新时间就获取更新时间 date = data.get('updated_time','')else: # 没有就获取创建时间 date = data.get('created_time','')timeArray = time.localtime(date)date = time.strftime("%Y-%m-%d %H:%M:%S", timeArray) 关于遇到的一些错误1 MySQLdb TypeError: %d format: a number is required, not str问题解决 这个其实就是传参数的时候占位符填错了,一般都是因为传int类型时不需要用%d,直接用%s就搞定了,亲测成功~! 还有一点就是最好要把str类型的占位符改成’%s’,就是带引号的这种,防止1146错误..]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>scrapy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[用twisted异步写入]]></title>
    <url>%2F2018%2F09%2F11%2Fuse_twisted%2F</url>
    <content type="text"><![CDATA[异步写入MySQL数据库,如果scrapy解析数据的速度远远超过数据库的写入速度,那么很容易就造成数据的丢失.这里用twisted保证数据的异步多线程写入,提高写入速度.详情请看正文@本文相关源码: https://github.com/Loveyueliang/yuke/tree/master/9-11/jobbolespider 首先需要先导入模块1 from twisted.enterprise import adbapi 接着在自定义的pipeline里面重构from_settings函数1 2 3 4 5 @classmethoddef from_settings(cls, settings): """ 这个函数名称是固定的，当爬虫启动的时候，scrapy会自动调用这些函数，加载配置数据。 """ 创建一个数据库连接池对象，这个连接池中可以包含多个connect链接对象。1 2 3 # 参数1：操作数据库的包名# 参数2：链接数据库的参数db_connect_pool = adbapi.ConnectionPool('pymysql', **params) 初始化这个类的对象1 obj = cls(db_connect_pool) 在process_item这个函数里执行数据的多线程写入操作1 2 3 4 5 # 参数1：在线程中被执行的sql语句# 参数2：要保存的数据result = self.dbpool.runInteraction(self.insert, item)# 给result绑定一个回调函数，用于监听错误信息result.addErrback(self.error) 定义一个打印错误信息的函数 1 2 def error(self, reason): print('--------', reason) 定义一个执行插入语句的函数,用twisted的好处其中一点就是不用一步步的手动提交 1 2 3 4 def insert(self, cursor, item): insert_sql = "INSERT INTO bole(bole_title, bole_date, bole_tag, bole_content, bole_dz, bole_sc, bole_pl, bole_img_src) VALUES (%s, %s, %s, %s, %s, %s, %s, %s)" cursor.execute(insert_sql, (item['bole_title'], item['bole_date'], item['bole_tag'], item['bole_content'], item['bole_dz'], item['bole_sc'], item['bole_pl'], item['bole_img_path'])) # 不需要commit()]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>scrapy,mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于51job的爬虫说明]]></title>
    <url>%2F2018%2F09%2F10%2Ftest_pic%2F</url>
    <content type="text"><![CDATA[@本文源码：https://github.com/Loveyueliang/yuke/tree/master/9-10 错误点: 感觉唯一的坑就是刚学的sql插入,一开始定义的表的字段名和item定义的不一样,所以就各种报KeyError.1 2 insert_sql = &quot;INSERT INTO job(zwmc, zwxz, gzdd, gzyq)VALUE (&apos;%s&apos;, &apos;%s&apos;, &apos;%s&apos;, &apos;%s&apos;)&quot;%(item[&apos;zwmc&apos;],item[&apos;zwxz&apos;],item[&apos;gzdd&apos;],item[&apos;gzyq&apos;]) self.cursor.execute(insert_sql) 最终结果还不错,暂时就爬取这一点信息吧.]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>scrapy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[xieyuying]]></title>
    <url>%2F2018%2F09%2F07%2Fxieyuying%2F</url>
    <content type="text"><![CDATA[@本文源码：https://github.com/Loveyueliang/XieYuYing.git si: 这里面写一写个人需求或建议的一写东西.调用剧情,选择用1-11这种类似的东西关于人物类的创建,人物可以考虑做在一个文件里,共同继承一个父类People –现在这一项已经完成.关于玩家名字的说明: 调用更改玩家名字的时候, 直接用赋值语句,现有判定输入只能为中文名字关于好感度的说明:好感度设计的逻辑一开始没有理清.现已完成 1.0版本–定义了一个好感度的类,方法为增加/减少好感度,目前是女主和胖老板的父类.理想参数为执行的相应动作,返回值为增加/减少的好感度. 但是目前还没有定义需要的所需要的以动作和数字为键值对的字典.动作多的话倒是可以采用这种方法, 要是很少的几个动作,估计要重新给每一个动作编写一个方法 好感度采用等差数列递增的方式.多引用一个增长速度 2.0最新进度,可以采用函数右移的方法来实现偶尔的坏话处理,,把函数做成列表生成器,达到每调用一次就可以得到下一个结果 生成器也做好了,但是需要一个初始的好感度,也就是说,,先给每个人用生成器生成到相应想给到的好感度 3.0最终版本:终于搞定了 好感度最终定义为一个列表增加好感度的方法里面先查询现在好感度的索引,然后通过索引加1的方法调用下一个应该得到的好感度. 关于商品的说明: 现在还没有开始做不过估计要创建一个商品的类,方法的话,应该就只有购买,不过属性可能会有点多. 需要注意: 要添加的一个功能–通过购买可以选择删除剧情中的某个人物,感觉关联度有点高]]></content>
      <categories>
        <category>类的使用</category>
      </categories>
      <tags>
        <tag>面向对象</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机开机]]></title>
    <url>%2F2018%2F09%2F06%2Fcapture_start%2F</url>
    <content type="text"><![CDATA[计算机启动过程 BIOS简介: 基本的输入输出系统,连接底层的硬件和系统 第一步: 按下电源键 按下电源键的时候电源开始向主板和cpu供电,一开始电压不稳定,主板控制芯片会一直向cpu发送reset信号,让cpu初始化 一般这个过程非常短暂,几乎在一瞬间电压就会稳定,控制芯片撤掉reset信号.但按下reset的时候,这个过程就特别明显…. 第二步: BIOS硬件自检–检测关键性设备 BIOS开始上电自检,这个地方会主要会检测内存,显卡,要是检测不通过,蜂鸣器会报告检测到的错误,一般用滴的长短和次数来表示错误的类型 这个地方检测的内存地址只有640k. 第三步: 屏幕亮,检测显卡BOIS 这里系统BIOS会读取显卡的BIOS启动显卡,此时屏幕开始亮起,会简单的显示显卡的一些参数, 要是不亮,就… 第四步: 显示BIOS自己的界面 会显示BIOS日期,主板芯片组型号,主板识别码,主板厂商,等一堆东西. 第五步: 仔细检测cpu和内存 会把检测到的cpu类型,主频打印在屏幕上 下面会跟着开始检测所有内存信息 第六步: 检测标准硬件设备 检测硬盘 cd-rom 软驱 并行接口,和串行接口 第七步: 检测外围设备 检测那些可以即插即用的东西,比如usb接口的U盘什么的.检测到后为每一个检测到的设备分配dma.i/o接口等 每检测到一个设备,都会把信息打印在屏幕上 第八步: BIOS清屏,重新显示系统配置表 会在系统上方显示一个概括性的配置列表,列出了系统安装的各种各种标准硬件设备以及他们使用的资源和相关参数. 第九步: 检测硬件更新 这里的硬件指的是类似外接内存,声卡,网卡,显卡,等设备有没有被更换,如果更换了,就更新一下存储,方面系统读取配置. 第十步: 根据启动顺序,启动引导 这一步,才开始引导操作系统,系统BIOS读取并执行硬盘的主引导记录,从分区表中找到第一个活动分区,然后读取并执行分区的引导记录.分区的引导记录负责读取并执行系统 下面是结构图]]></content>
      <categories>
        <category>前端</category>
      </categories>
      <tags>
        <tag>capture</tag>
      </tags>
  </entry>
</search>
